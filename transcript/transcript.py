#! /Users/aaronyang/miniforge3/envs/coursea/bin/python

"""
todo: 

1. åœ¨è¿›è¡Œç¬¬ä¸€è½®transcriptä¹‹å‰ï¼Œå°±å¯¹è¾“å…¥è§†é¢‘è¿›è¡Œåˆ‡åˆ†(a)ï¼Œä½¿å¾—æ¯ä¸€æ®µä¸é•¿äº15åˆ†é’Ÿã€‚å°†åˆ†å‰²çš„æ–‡ä»¶ä¿å­˜åˆ° /tmp/$working_dir/cuts
2. å¯¹cutsé‡Œçš„æ¯ä¸€ä¸ªæ–‡ä»¶ï¼Œè¿›è¡Œtranscriptï¼Œç„¶åç­‰å¾…äººå·¥æ ¡å¯¹
3. äººå·¥æ ¡æ­£çš„srtå¯èƒ½åŒ…å«ç¼–è¾‘æŒ‡ä»¤ï¼Œæ¯”å¦‚[del],å’Œè‡ªåŠ¨åˆ é™¤å•è¡Œçš„ã€å‘ƒã€æ©ã€å¯¹ï¼Œå¥½ã€ç­‰ï¼Œè¿™æ ·ä¼šç”Ÿæˆä¸€äº›å°çš„ç¢ç‰‡ï¼Œé€šè¿‡ffmpegè¿›è¡Œå‰ªè¾‘ã€åˆå¹¶ã€‚è¿™ä¸€æ­¥çš„ç»“æœä¿å­˜åˆ°/tmp/$working_dir/reviewed
4. å°†reviewedçš„æ–‡ä»¶ã€åŠ å…¥ç‰‡å¤´ç‰‡å°¾ï¼Œåˆå¹¶ã€å‹ç¼©å’Œä¿å­˜
5. å°†reviewedä¸­çš„æ–‡ä»¶ï¼Œçƒ§å…¥å­—å¹•ï¼ŒåŠ ç‰‡å¤´ç‰‡å°¾ï¼Œåˆå¹¶ã€å‹ç¼©å’Œä¿å­˜


å…¶å®ƒï¼š
1. å¯¹1ä¸­äº§ç”Ÿçš„cutsï¼Œæ¯å¤„ç†ä¸€ä¸ªï¼Œéƒ½è¦æ˜¾ç¤º æ­£åœ¨å¤„ç† i/total åŠ å·²å®Œæˆ

å‚è€ƒï¼š
a: è¿™ä¸ªå‘½ä»¤èƒ½æŒ‰å…³é”®å¸§åˆ†å‰²ï¼š ffmpeg -i raw.mp4 -c copy -map 0 -segment_time 00:15:00 -f segment -break_non_keyframes 0 output%03d.mp4

"""

import datetime
import json
import os
import shlex
import shutil
import subprocess
import sys
import warnings
from multiprocessing.util import is_exiting
from pathlib import Path
from subprocess import PIPE, STDOUT, Popen
from typing import Any, List, Tuple

# fireå·²ç§»é™¤ï¼Œä½¿ç”¨CLIæ¥å£
import jieba
import opencc
import pysubs2

# from pywhispercpp.model import Model


warnings.filterwarnings("ignore")

import whisperx


def detect_optimal_device_config():
    """æ£€æµ‹å¹¶é…ç½®æœ€ä¼˜çš„è®¾å¤‡å’Œè®¡ç®—ç±»å‹ï¼ˆä¸“ä¸ºM1/M4ä¼˜åŒ–ï¼‰"""
    import platform
    import subprocess

    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰ç¯å¢ƒå˜é‡è¦†ç›–
    env_device = os.environ.get('WHISPERX_DEVICE')
    env_compute_type = os.environ.get('WHISPERX_COMPUTE_TYPE')

    if env_device and env_compute_type:
        print(f"ğŸ”§ ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®: device={env_device}, compute_type={env_compute_type}")
        return env_device, env_compute_type

    # æ£€æµ‹ç³»ç»Ÿä¿¡æ¯
    system = platform.system()
    machine = platform.machine()

    if system == "Darwin" and machine == "arm64":
        # Apple Silicon Mac - ä¸“ä¸ºM1/M4ä¼˜åŒ–
        try:
            result = subprocess.run(['sysctl', '-n', 'machdep.cpu.brand_string'],
                                  capture_output=True, text=True)
            cpu_info = result.stdout.strip()

            if "M1" in cpu_info:
                print("âš¡ M1èŠ¯ç‰‡ï¼šä½¿ç”¨CPUä¼˜åŒ–é…ç½®ï¼ˆç»å®æµ‹éªŒè¯æœ€ä¼˜ï¼‰")
                device = "cpu"
                compute_type = "int8"
                # M1ä¸“ç”¨ä¼˜åŒ–
                os.environ.setdefault('BLAS_VENDOR', 'Apple')
                os.environ.setdefault('LAPACK_VENDOR', 'Apple')
                os.environ.setdefault('VECLIB_MAXIMUM_THREADS', '8')
                os.environ.setdefault('WHISPERX_BATCH_SIZE', '8')
                os.environ.setdefault('WHISPERX_CHUNK_SIZE', '5')  # å‡å°chunk_sizeä»¥è·å¾—æ›´çŸ­çš„ç‰‡æ®µ

            elif "M4" in cpu_info or "M3" in cpu_info or "M2" in cpu_info:
                print("âš¡ M4/M3/M2èŠ¯ç‰‡ï¼šä½¿ç”¨CPUä¼˜åŒ–é…ç½®ï¼ˆæ¨èï¼‰")
                device = "cpu"
                compute_type = "int8"
                # M4ä¸“ç”¨ä¼˜åŒ–
                os.environ.setdefault('BLAS_VENDOR', 'Apple')
                os.environ.setdefault('LAPACK_VENDOR', 'Apple')
                os.environ.setdefault('VECLIB_MAXIMUM_THREADS', '12')
                os.environ.setdefault('WHISPERX_BATCH_SIZE', '16')
                os.environ.setdefault('WHISPERX_CHUNK_SIZE', '5')  # å‡å°chunk_sizeä»¥è·å¾—æ›´çŸ­çš„ç‰‡æ®µ

                # å¼ºåˆ¶ç¦ç”¨MPSï¼Œå› ä¸ºWhisperXå¯¹MPSæ”¯æŒä¸å®Œæ•´
                print("âš ï¸ å¼ºåˆ¶ç¦ç”¨MPSè®¾å¤‡ï¼Œä½¿ç”¨CPUä»¥ç¡®ä¿å…¼å®¹æ€§")
                os.environ['CUDA_VISIBLE_DEVICES'] = ''  # ç¦ç”¨CUDA
                os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'  # å¯ç”¨MPSå›é€€
            else:
                # é»˜è®¤M1é…ç½®
                device = "cpu"
                compute_type = "int8"
                os.environ.setdefault('WHISPERX_BATCH_SIZE', '8')
                os.environ.setdefault('WHISPERX_CHUNK_SIZE', '5')  # å‡å°chunk_sizeä»¥è·å¾—æ›´çŸ­çš„ç‰‡æ®µ

        except Exception:
            # é»˜è®¤é…ç½®
            device = "cpu"
            compute_type = "int8"
            os.environ.setdefault('WHISPERX_BATCH_SIZE', '8')
            os.environ.setdefault('WHISPERX_CHUNK_SIZE', '5')  # å‡å°chunk_sizeä»¥è·å¾—æ›´çŸ­çš„ç‰‡æ®µ

        # é€šç”¨Appleä¼˜åŒ–
        os.environ.setdefault('OMP_NUM_THREADS', '8')
        os.environ.setdefault('MKL_NUM_THREADS', '8')

    else:
        # éApple Siliconç³»ç»Ÿ
        device = "cpu"
        compute_type = "int8"
        os.environ.setdefault('WHISPERX_BATCH_SIZE', '8')
        os.environ.setdefault('WHISPERX_CHUNK_SIZE', '5')  # å‡å°chunk_sizeä»¥è·å¾—æ›´çŸ­çš„ç‰‡æ®µ

    return device, compute_type


# å¼ºåˆ¶ç¦ç”¨MPSä»¥ç¡®ä¿WhisperXå…¼å®¹æ€§
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'

# åœ¨æ¨¡å—åŠ è½½æ—¶æ£€æµ‹æœ€ä¼˜é…ç½®
optimal_device, optimal_compute_type = detect_optimal_device_config()

# å†æ¬¡ç¡®ä¿è®¾å¤‡é…ç½®ä¸ºCPUï¼ˆé˜²æ­¢ä»»ä½•MPSç›¸å…³é—®é¢˜ï¼‰
if optimal_device == "mps":
    print("âš ï¸ æ£€æµ‹åˆ°MPSè®¾å¤‡é…ç½®ï¼Œå¼ºåˆ¶åˆ‡æ¢åˆ°CPUä»¥ç¡®ä¿å…¼å®¹æ€§")
    optimal_device = "cpu"
    optimal_compute_type = "int8"

# ä½¿ç”¨æ›´ä¸å®¹æ˜“è¢«è¯¯è¯†åˆ«çš„prompt
prompt = "ä»¥ä¸‹æ˜¯ä¸­æ–‡éŸ³é¢‘è½¬å½•ï¼š"

opening_video = Path("/Volumes/share/data/autobackup/ke/factor-ml/opening.mp4")
ending_video = Path("/Volumes/share/data/autobackup/ke/factor-ml/end.mp4")

cpp_path = Path("/Volumes/share/data/whisper.cpp")
cpp_model = Path("/Volumes/share/data/whisper.cpp/models/ggml-large-v2.bin")
# ä½¿ç”¨HF_HOMEç¯å¢ƒå˜é‡è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•
hf_home = os.environ.get("HF_HOME", "/Volumes/share/data/models/huggingface")
model_dir = os.path.join(hf_home, "hub")

# è®¾ç½®whisperxæ¨¡å‹åç§°
whisperx_model = "large-v2"  # æ”¯æŒä¸­æ–‡çš„whisperæ¨¡å‹
w2v_model = "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn"  # ä¸­æ–‡å¯¹é½æ¨¡å‹

def align_subtitles_with_audio(video: Path, original_srt: Path, aligned_srt: Path):
    """
    ä½¿ç”¨ whisperx å¯¹é½å­—å¹•æ–‡ä»¶ä¸éŸ³é¢‘ã€‚
    è¿™æ˜¯ç¡®ä¿å‰ªè¾‘åè§†é¢‘ä¸å­—å¹•åŒæ­¥çš„å…³é”®æ­¥éª¤ã€‚

    Args:
        video: åˆå¹¶åçš„è§†é¢‘æ–‡ä»¶è·¯å¾„
        original_srt: åŸå§‹å­—å¹•æ–‡ä»¶è·¯å¾„
        aligned_srt: å¯¹é½åçš„å­—å¹•æ–‡ä»¶è·¯å¾„
    """
    print(f"å¼€å§‹å­—å¹•å¯¹é½: {original_srt} -> {aligned_srt}")

    try:
        # éªŒè¯è¾“å…¥æ–‡ä»¶
        if not Path(video).exists():
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video}")
        if not Path(original_srt).exists():
            raise FileNotFoundError(f"å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {original_srt}")

        # åˆ›å»ºä¸“ç”¨çš„16kHzå•å£°é“éŸ³é¢‘æ–‡ä»¶ç”¨äºå¯¹é½ï¼ˆä¸è¦†ç›–åŸæ–‡ä»¶ï¼‰
        video_path = Path(video)
        audio_path = video_path.parent / f"{video_path.stem}_alignment.wav"
        print(f"ğŸ“ ä¸ºå¯¹é½åˆ›å»º16kHzå•å£°é“éŸ³é¢‘: {audio_path.name}")
        ensure_16khz_mono_wav(video_path, audio_path, force_convert=True)

        # è®¾ç½®è®¾å¤‡ - Mac ARMä¼˜åŒ–
        import platform
        if platform.system() == "Darwin" and platform.machine() == "arm64":
            # Mac ARMæ¶æ„ï¼Œä½¿ç”¨CPU
            device = "cpu"
            print("æ£€æµ‹åˆ°Mac ARMæ¶æ„ï¼Œä½¿ç”¨CPUè¿›è¡Œå¯¹é½")
        else:
            device = "cpu"  # ä¿æŒå…¼å®¹æ€§

        print("åŠ è½½éŸ³é¢‘...")
        audio = whisperx.load_audio(str(audio_path))

        # åŠ è½½å­—å¹•æ–‡ä»¶
        print("åŠ è½½å­—å¹•æ–‡ä»¶...")
        subs = pysubs2.load(str(original_srt))

        if not subs.events:
            print("è­¦å‘Š: å­—å¹•æ–‡ä»¶ä¸ºç©º")
            shutil.copy2(original_srt, aligned_srt)
            return

        segments = []
        for i, event in enumerate(subs.events):
            if event.text.strip():  # è·³è¿‡ç©ºå­—å¹•
                segments.append({
                    "start": event.start / 1000,
                    "end": event.end / 1000,
                    "text": event.text.strip(),
                    "id": i
                })

        if not segments:
            print("è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„å­—å¹•æ®µè½")
            shutil.copy2(original_srt, aligned_srt)
            return

        # è®¾ç½®ç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡
        import os
        os.environ["TRANSFORMERS_OFFLINE"] = "1"
        os.environ["HF_HUB_OFFLINE"] = "1"

        # åŠ è½½å¯¹é½æ¨¡å‹
        print("åŠ è½½å¯¹é½æ¨¡å‹...")
        model_name = None

        # å°è¯•ä½¿ç”¨HF_HOMEç¼“å­˜è·¯å¾„
        local_model_path = Path(model_dir) / "models--jonatasgrosman--wav2vec2-large-xlsr-53-chinese-zh-cn"
        if local_model_path.exists():
            snapshots_dir = local_model_path / "snapshots"
            if snapshots_dir.exists():
                snapshot_dirs = [d for d in snapshots_dir.iterdir() if d.is_dir()]
                if snapshot_dirs:
                    latest_snapshot = max(snapshot_dirs, key=lambda x: x.stat().st_mtime)
                    print(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {latest_snapshot}")
                    model_name = str(latest_snapshot)

        if model_name is None:
            model_name = w2v_model
            print(f"ä½¿ç”¨é»˜è®¤æ¨¡å‹åç§°: {model_name}")

        try:
            # ä½¿ç”¨HF_HOMEä½œä¸ºç¼“å­˜ç›®å½•
            model_a, metadata = whisperx.load_align_model(
                language_code="zh",
                device=device,
                model_name=model_name,
                model_dir=hf_home
            )
            print("å¯¹é½æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¼€å§‹å¯¹é½...")
        except Exception as model_error:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {model_error}")
            raise

        # æ‰§è¡Œå¯¹é½
        print(f"å¯¹é½ {len(segments)} ä¸ªå­—å¹•æ®µè½...")
        aligned_result = whisperx.align(segments, model_a, metadata, audio, device)

        if "segments" not in aligned_result or not aligned_result["segments"]:
            print("è­¦å‘Š: å¯¹é½ç»“æœä¸ºç©º")
            shutil.copy2(original_srt, aligned_srt)
            return

        # åˆ›å»ºå¯¹é½åçš„å­—å¹•
        aligned_subs = pysubs2.SSAFile()

        for i, segment in enumerate(aligned_result["segments"]):
            if "start" in segment and "end" in segment and "text" in segment:
                event = pysubs2.SSAEvent()
                event.start = int(segment["start"] * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                event.end = int(segment["end"] * 1000)
                event.text = segment["text"]

                # ç¡®ä¿æ—¶é—´æˆ³åˆç†
                if event.end <= event.start:
                    event.end = event.start + 1000  # è‡³å°‘1ç§’

                # é¿å…é‡å 
                if i > 0 and event.start < aligned_subs.events[-1].end:
                    event.start = aligned_subs.events[-1].end + 100
                    if event.end <= event.start:
                        event.end = event.start + 1000

                aligned_subs.events.append(event)

        # ä¿å­˜å¯¹é½åçš„å­—å¹•æ–‡ä»¶
        aligned_subs.save(str(aligned_srt))
        print(f"âœ… å­—å¹•å¯¹é½å®Œæˆ: {len(aligned_subs.events)} ä¸ªæ®µè½")
        print(f"å¯¹é½ç»“æœä¿å­˜åˆ°: {aligned_srt}")

    except Exception as e:
        print(f"âŒ å­—å¹•å¯¹é½å¤±è´¥: {e}")
        print("ä½¿ç”¨åŸå§‹å­—å¹•æ–‡ä»¶ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ...")

        # ç›´æ¥å¤åˆ¶åŸå§‹å­—å¹•æ–‡ä»¶
        shutil.copy2(original_srt, aligned_srt)
        print(f"å·²å¤åˆ¶åŸå§‹å­—å¹•åˆ°: {aligned_srt}")



def execute(cmd, dry_run=False, supress_log=False, msg: str = ""):
    start = datetime.datetime.now()
    if not supress_log:
        print(f">>> {msg} {cmd}")

    if dry_run:
        return

    if isinstance(cmd, str):
        args = shlex.split(cmd)
    else:
        args = cmd
    with Popen(args, stdout=PIPE, stderr=STDOUT, text=True) as proc:
        for line in proc.stdout:  # type: ignore
            print(line)

    if proc.returncode != 0:
        raise RuntimeError(f"FFmpeg Error {proc.returncode}: {proc.stderr}")

    if not supress_log:
        cost(start, prefix=f"<<< {msg} Done ")


def _ms_to_hms(ms: int):
    ms_ = ms / 1000
    h = int(ms_ // 3600)
    m = int((ms_ - h * 3600) // 60)
    s = int((ms_ - h * 3600 - m * 60))

    return f"{h:02d}:{m:02d}:{s:02d}.{ms % 1000:03d}"


def transcript_cpp(input_audio: Path, output_srt: Path, prompt: str, dry_run=False):
    """ä½¿ç”¨whisper.cppè¿›è¡ŒéŸ³é¢‘è½¬å½•ï¼ˆä»…è½¬å½•ï¼Œä¸å«è¯´è¯äººåˆ†ç¦»ï¼‰

    Args:
        input_audio: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        output_srt: è¾“å‡ºå­—å¹•æ–‡ä»¶è·¯å¾„
        prompt: è½¬å½•æç¤ºè¯
        dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼
    """
    print(f"ä½¿ç”¨whisper.cppè½¬å½•éŸ³é¢‘: {input_audio} -> {output_srt}")

    if dry_run:
        print("è¯•è¿è¡Œæ¨¡å¼ï¼Œè·³è¿‡å®é™…è½¬å½•")
        return

    whisper = os.path.join(cpp_path, "whisper-cli")
    cmd = f"{whisper} {input_audio} -l zh -sow -ml 30 -t 8 -m {cpp_model} -osrt -of {output_srt.with_suffix('')} --prompt '{prompt}'"

    try:
        execute(cmd)
    except Exception as e:
        print(f"âŒ whisper.cppè½¬å½•å¤±è´¥: {e}")
        raise



def create_speaker_text_file(srt_file: Path, output_txt: Path):
    """
    ä»SRTæ–‡ä»¶åˆ›å»ºå¸¦è¯´è¯äººæ ‡è¯†çš„æ™®é€šæ–‡æœ¬æ–‡ä»¶

    Args:
        srt_file: è¾“å…¥çš„SRTå­—å¹•æ–‡ä»¶
        output_txt: è¾“å‡ºçš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„
    """
    try:
        subs = pysubs2.load(str(srt_file))

        with open(output_txt, 'w', encoding='utf-8') as f:
            for event in subs.events:
                text = event.text.strip()
                if text:
                    # æ£€æŸ¥æ˜¯å¦å·²ç»åŒ…å«è¯´è¯äººæ ‡è¯†
                    if text.startswith('[') and ']' in text:
                        # å·²ç»æœ‰è¯´è¯äººæ ‡è¯†ï¼Œç›´æ¥å†™å…¥
                        f.write(f"{text}\n")
                    else:
                        # æ²¡æœ‰è¯´è¯äººæ ‡è¯†ï¼Œæ·»åŠ é»˜è®¤æ ‡è¯†
                        f.write(f"[è¯´è¯äºº] {text}\n")

        print(f"âœ… å¸¦è¯´è¯äººæ ‡è¯†çš„æ–‡æœ¬æ–‡ä»¶å·²ç”Ÿæˆ: {output_txt}")

    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¸¦è¯´è¯äººæ ‡è¯†æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {e}")
        # åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶ä»¥é¿å…åç»­é”™è¯¯
        with open(output_txt, 'w', encoding='utf-8') as f:
            f.write("")


def create_clean_srt_file(srt_file: Path, output_srt: Path):
    """
    ä»å¸¦è¯´è¯äººæ ‡è¯†çš„SRTæ–‡ä»¶åˆ›å»ºä¸å¸¦è¯´è¯äººæ ‡è¯†çš„å¹²å‡€SRTæ–‡ä»¶

    Args:
        srt_file: è¾“å…¥çš„SRTå­—å¹•æ–‡ä»¶ï¼ˆå¯èƒ½åŒ…å«è¯´è¯äººæ ‡è¯†ï¼‰
        output_srt: è¾“å‡ºçš„å¹²å‡€SRTæ–‡ä»¶è·¯å¾„
    """
    try:
        subs = pysubs2.load(str(srt_file))
        clean_subs = pysubs2.SSAFile()

        for event in subs.events:
            text = event.text.strip()
            if text:
                # ç§»é™¤è¯´è¯äººæ ‡è¯†
                if text.startswith('[') and ']' in text:
                    # æ‰¾åˆ°ç¬¬ä¸€ä¸ª']'çš„ä½ç½®
                    bracket_end = text.find(']')
                    if bracket_end != -1:
                        # æå–è¯´è¯äººæ ‡è¯†åçš„å†…å®¹
                        clean_text = text[bracket_end + 1:].strip()
                    else:
                        clean_text = text
                else:
                    clean_text = text

                # åˆ›å»ºæ–°çš„äº‹ä»¶
                if clean_text:
                    new_event = pysubs2.SSAEvent(
                        start=event.start,
                        end=event.end,
                        text=clean_text
                    )
                    clean_subs.events.append(new_event)

        clean_subs.save(str(output_srt))
        print(f"âœ… å¹²å‡€çš„SRTæ–‡ä»¶å·²ç”Ÿæˆ: {output_srt}")

    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¹²å‡€SRTæ–‡ä»¶å¤±è´¥: {e}")
        # å¤åˆ¶åŸæ–‡ä»¶ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
        import shutil
        shutil.copy2(srt_file, output_srt)


def init_jieba():
    """
    åˆå§‹åŒ–jiebaåˆ†è¯å™¨å¹¶åŠ è½½è‡ªå®šä¹‰è¯å…¸

    Returns:
        tuple: (replace_map, warning_words) - æ›¿æ¢è¯å…¸å’Œè­¦å‘Šè¯åˆ—è¡¨
    """
    import yaml

    replace_map = {}
    warning_words = []

    # å°è¯•åŠ è½½YAMLæ ¼å¼çš„å­—å…¸æ–‡ä»¶
    dict_path = Path(__file__).parent.parent / "dict.yaml"

    if dict_path.exists():
        try:
            with open(dict_path, "r", encoding="utf-8") as f:
                dict_data = yaml.safe_load(f)

            # åŠ è½½æ›¿æ¢è¯å…¸
            if "replace" in dict_data and dict_data["replace"]:
                replace_map = dict_data["replace"]
                print(f"ğŸ“š åŠ è½½æ›¿æ¢è¯å…¸: {len(replace_map)} ä¸ªè¯æ±‡")

            # åŠ è½½è­¦å‘Šè¯åˆ—è¡¨
            if "warning" in dict_data and dict_data["warning"]:
                warning_words = dict_data["warning"]
                print(f"âš ï¸  åŠ è½½è­¦å‘Šè¯åˆ—è¡¨: {len(warning_words)} ä¸ªè¯æ±‡")

        except Exception as e:
            print(f"âŒ åŠ è½½YAMLå­—å…¸å¤±è´¥: {e}")
            print("å°è¯•åŠ è½½æ—§æ ¼å¼å­—å…¸...")

            # å›é€€åˆ°æ—§æ ¼å¼
            old_dict_path = Path(__file__).parent.parent / "words.md"
            if old_dict_path.exists():
                with open(old_dict_path, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if line.startswith("#") or len(line) == 0:
                            continue
                        parts = line.split(",")
                        if len(parts) >= 2:
                            wrong, right = parts[0], parts[1]
                            replace_map[wrong] = right
                print(f"ğŸ“š ä»æ—§æ ¼å¼åŠ è½½: {len(replace_map)} ä¸ªè¯æ±‡")
    else:
        print(f"âš ï¸  å­—å…¸æ–‡ä»¶ä¸å­˜åœ¨: {dict_path}")

    # å°†æ‰€æœ‰è¯æ±‡æ·»åŠ åˆ°jiebaè¯å…¸
    for word in replace_map.keys():
        jieba.add_word(word)

    for word in warning_words:
        jieba.add_word(word)

    return replace_map, warning_words


def transcriptx(input_audio: Path, output_srt: Path, prompt: str):
    """ä½¿ç”¨whisperxè¿›è¡ŒéŸ³é¢‘è½¬å½•ï¼Œæ”¯æŒApple Siliconä¼˜åŒ–"""
    print(f"ä½¿ç”¨whisperxè½¬å½•éŸ³é¢‘: {input_audio} -> {output_srt}")

    # ä½¿ç”¨æ£€æµ‹åˆ°çš„æœ€ä¼˜é…ç½®
    device = optimal_device
    compute_type = optimal_compute_type

    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡é…ç½®: {device} (compute_type: {compute_type})")

    try:
        options = {"initial_prompt": prompt}
        print("åŠ è½½whisperxæ¨¡å‹...")
        try:
            # ä½¿ç”¨HF_HOMEç¯å¢ƒå˜é‡è®¾ç½®çš„ç¼“å­˜ç›®å½•
            download_root = hf_home
            local_files_only = os.environ.get('HF_HUB_OFFLINE', '0') == '1'

            print(f"ğŸ”§ æ¨¡å‹ç¼“å­˜ç›®å½•: {download_root}")
            print(f"ğŸ”§ ç¦»çº¿æ¨¡å¼: {local_files_only}")

            model = whisperx.load_model(
                whisperx_model,
                device=device,
                compute_type=compute_type,
                asr_options=options,
                language="zh",
                threads=8,
                download_root=download_root,
                local_files_only=local_files_only
            )
        except Exception as model_error:
            print(f"âš ï¸ åŠ è½½whisperxæ¨¡å‹å¤±è´¥: {model_error}")
            print("å°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å‹æˆ–é™çº§æ¨¡å‹...")
            # å°è¯•ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹
            try:
                model = whisperx.load_model(
                    "base",  # ä½¿ç”¨åŸºç¡€æ¨¡å‹
                    device=device,
                    compute_type=compute_type,
                    asr_options=options,
                    language="zh",
                    threads=8,
                    local_files_only=False
                )
                print("âœ… æˆåŠŸåŠ è½½åŸºç¡€æ¨¡å‹")
            except Exception as fallback_error:
                print(f"âŒ åŸºç¡€æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {fallback_error}")
                raise model_error

        print("åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
        audio = whisperx.load_audio(str(input_audio))

        print("å¼€å§‹è½¬å½•...")

        # ä»ç¯å¢ƒå˜é‡è·å–æ‰¹å¤„ç†é…ç½®
        batch_size = int(os.environ.get('WHISPERX_BATCH_SIZE', '8'))
        chunk_size = int(os.environ.get('WHISPERX_CHUNK_SIZE', '10'))

        print(f"ğŸ”§ è½¬å½•å‚æ•°: batch_size={batch_size}, chunk_size={chunk_size}")

        result = model.transcribe(
            audio, language="zh", print_progress=True,
            batch_size=batch_size, chunk_size=chunk_size
        )

        if "segments" not in result or not result["segments"]:
            print("âš ï¸ è½¬å½•ç»“æœä¸ºç©ºï¼Œåˆ›å»ºç©ºå­—å¹•æ–‡ä»¶")
            # åˆ›å»ºä¸€ä¸ªç©ºçš„å­—å¹•æ–‡ä»¶
            empty_subs = pysubs2.SSAFile()
            empty_subs.save(str(output_srt))
        else:
            print(f"è½¬å½•å®Œæˆï¼Œå…± {len(result['segments'])} ä¸ªç‰‡æ®µ")
            subs = pysubs2.load_from_whisper(result["segments"])
            subs.save(str(output_srt))
            print(f"å­—å¹•æ–‡ä»¶å·²ä¿å­˜: {output_srt}")

    except Exception as e:
        print(f"âŒ whisperxè½¬å½•å¤±è´¥: {e}")
        # åˆ›å»ºä¸€ä¸ªç©ºçš„å­—å¹•æ–‡ä»¶ä»¥é¿å…åç»­é”™è¯¯
        print("åˆ›å»ºç©ºå­—å¹•æ–‡ä»¶ä»¥é¿å…åç»­é”™è¯¯...")
        empty_subs = pysubs2.SSAFile()
        empty_subs.save(str(output_srt))
        raise
    # 2. Align whisper output
    # model_a, metadata = whisperx.load_align_model(
    #     language_code=result["language"], device=device, model_name = w2v_model
    # )
    # result = whisperx.align(
    #     result["segments"],
    #     model_a,
    #     metadata,
    #     audio,
    #     device,
    #     return_char_alignments=False,
    # )

    # print(result["segments"])  # after alignment


def transcriptx_with_diarization(input_audio: Path, output_srt: Path, prompt: str):
    """ä½¿ç”¨whisperxè¿›è¡ŒéŸ³é¢‘è½¬å½•ï¼Œæ”¯æŒè¯´è¯äººåˆ†ç¦»å’ŒApple Siliconä¼˜åŒ–"""
    print(f"ä½¿ç”¨whisperxè½¬å½•éŸ³é¢‘ï¼ˆå«è¯´è¯äººåˆ†ç¦»ï¼‰: {input_audio} -> {output_srt}")
    print("ğŸ­ å¯ç”¨è¯´è¯äººåˆ†ç¦»åŠŸèƒ½")

    # ä½¿ç”¨æ£€æµ‹åˆ°çš„æœ€ä¼˜é…ç½®
    device = optimal_device
    compute_type = optimal_compute_type

    print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡é…ç½®: {device} (compute_type: {compute_type})")

    try:
        options = {"initial_prompt": prompt}
        print("åŠ è½½whisperxæ¨¡å‹...")
        try:
            # ä½¿ç”¨HF_HOMEç¯å¢ƒå˜é‡è®¾ç½®çš„ç¼“å­˜ç›®å½•
            download_root = hf_home
            local_files_only = os.environ.get('HF_HUB_OFFLINE', '0') == '1'

            print(f"ğŸ”§ æ¨¡å‹ç¼“å­˜ç›®å½•: {download_root}")
            print(f"ğŸ”§ ç¦»çº¿æ¨¡å¼: {local_files_only}")

            model = whisperx.load_model(
                whisperx_model,
                device=device,
                compute_type=compute_type,
                asr_options=options,
                language="zh",
                threads=8,
                download_root=download_root,
                local_files_only=local_files_only
            )
        except Exception as model_error:
            print(f"âš ï¸ åŠ è½½whisperxæ¨¡å‹å¤±è´¥: {model_error}")
            print("å°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å‹æˆ–é™çº§æ¨¡å‹...")
            # å°è¯•ä½¿ç”¨æ›´ç®€å•çš„æ¨¡å‹
            try:
                model = whisperx.load_model(
                    "base",  # ä½¿ç”¨åŸºç¡€æ¨¡å‹
                    device=device,
                    compute_type=compute_type,
                    asr_options=options,
                    language="zh",
                    threads=8,
                    local_files_only=False
                )
                print("âœ… æˆåŠŸåŠ è½½åŸºç¡€æ¨¡å‹")
            except Exception as fallback_error:
                print(f"âŒ åŸºç¡€æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {fallback_error}")
                raise model_error

        print("åŠ è½½éŸ³é¢‘æ–‡ä»¶...")
        audio = whisperx.load_audio(str(input_audio))

        print("å¼€å§‹è½¬å½•...")

        # ä»ç¯å¢ƒå˜é‡è·å–æ‰¹å¤„ç†é…ç½®
        batch_size = int(os.environ.get('WHISPERX_BATCH_SIZE', '8'))
        chunk_size = int(os.environ.get('WHISPERX_CHUNK_SIZE', '10'))

        print(f"ğŸ”§ è½¬å½•å‚æ•°: batch_size={batch_size}, chunk_size={chunk_size}")

        result = model.transcribe(
            audio, language="zh", print_progress=True,
            batch_size=batch_size, chunk_size=chunk_size
        )

        if "segments" not in result or not result["segments"]:
            print("âš ï¸ è½¬å½•ç»“æœä¸ºç©ºï¼Œåˆ›å»ºç©ºå­—å¹•æ–‡ä»¶")
            empty_subs = pysubs2.SSAFile()
            empty_subs.save(str(output_srt))
            return

        print(f"è½¬å½•å®Œæˆï¼Œå…± {len(result['segments'])} ä¸ªç‰‡æ®µ")

        try:
            print("ğŸ”„ å¼€å§‹è¯´è¯äººåˆ†ç¦»...")

            # ç›´æ¥è¿›è¡Œè¯´è¯äººåˆ†ç¦»ï¼Œè·³è¿‡å¯¹é½æ­¥éª¤
            # æ³¨æ„ï¼šå¯¹é½å°†åœ¨ç”¨æˆ·ç¼–è¾‘å­—å¹•åçš„resumeé˜¶æ®µè¿›è¡Œ
            print("åŠ è½½è¯´è¯äººåˆ†ç¦»æ¨¡å‹...")
            try:
                # ä½¿ç”¨SpeechBrainè¿›è¡Œè¯´è¯äººåˆ†ç¦»
                subs = speechbrain_speaker_diarization(result["segments"], audio, input_audio)

            except ImportError as import_error:
                print(f"âŒ ç¼ºå°‘SpeechBrainä¾èµ–: {import_error}")
                print("è¯·å®‰è£…è¯´è¯äººåˆ†ç¦»ä¾èµ–:")
                print("pip install speechbrain")
                raise Exception("è¯´è¯äººåˆ†ç¦»éœ€è¦å®‰è£…speechbrain")

            except Exception as diarize_error:
                print(f"âŒ è¯´è¯äººåˆ†ç¦»å¤±è´¥: {diarize_error}")
                print("å¯èƒ½çš„åŸå› :")
                print("1. ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œæ— æ³•ä¸‹è½½æ¨¡å‹")
                print("2. éŸ³é¢‘æ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ")
                print("3. å†…å­˜ä¸è¶³")
                raise

        except Exception as diarize_error:
            print(f"âš ï¸ è¯´è¯äººåˆ†ç¦»å¤±è´¥: {diarize_error}")
            print("å›é€€åˆ°æ™®é€šè½¬å½•æ¨¡å¼ï¼ˆä¸å«è¯´è¯äººåˆ†ç¦»ï¼‰...")
            subs = pysubs2.load_from_whisper(result["segments"])

        subs.save(str(output_srt))
        print(f"å­—å¹•æ–‡ä»¶å·²ä¿å­˜: {output_srt}")

    except Exception as e:
        print(f"âŒ whisperxè½¬å½•å¤±è´¥: {e}")
        # åˆ›å»ºä¸€ä¸ªç©ºçš„å­—å¹•æ–‡ä»¶ä»¥é¿å…åç»­é”™è¯¯
        print("åˆ›å»ºç©ºå­—å¹•æ–‡ä»¶ä»¥é¿å…åç»­é”™è¯¯...")
        empty_subs = pysubs2.SSAFile()
        empty_subs.save(str(output_srt))
        raise





def get_audio_info(audio_file: Path):
    """
    è·å–éŸ³é¢‘æ–‡ä»¶çš„é‡‡æ ·ç‡å’Œå£°é“ä¿¡æ¯

    Returns:
        tuple: (sample_rate, channels) æˆ– (None, None) å¦‚æœæ£€æµ‹å¤±è´¥
    """
    try:
        import subprocess
        cmd = [
            "ffprobe", "-v", "quiet", "-print_format", "json", "-show_streams",
            str(audio_file)
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)

        if result.returncode == 0:
            import json
            data = json.loads(result.stdout)
            for stream in data.get("streams", []):
                if stream.get("codec_type") == "audio":
                    sample_rate = int(stream.get("sample_rate", 0))
                    channels = int(stream.get("channels", 0))
                    return sample_rate, channels
        return None, None
    except Exception as e:
        print(f"âš ï¸ è·å–éŸ³é¢‘ä¿¡æ¯å¤±è´¥: {e}")
        return None, None


def ensure_16khz_mono_wav(input_file: Path, output_wav: Path, force_convert=False):
    """
    ç¡®ä¿éŸ³é¢‘æ–‡ä»¶ä¸º16kHzå•å£°é“WAVæ ¼å¼

    Args:
        input_file: è¾“å…¥éŸ³é¢‘/è§†é¢‘æ–‡ä»¶
        output_wav: è¾“å‡ºWAVæ–‡ä»¶è·¯å¾„
        force_convert: æ˜¯å¦å¼ºåˆ¶è½¬æ¢ï¼ˆå³ä½¿å·²ç»æ˜¯æ­£ç¡®æ ¼å¼ï¼‰
    """
    need_convert = force_convert

    if output_wav.exists() and not force_convert:
        # æ£€æŸ¥ç°æœ‰æ–‡ä»¶çš„æ ¼å¼
        sample_rate, channels = get_audio_info(output_wav)
        if sample_rate == 16000 and channels == 1:
            print(f"âœ… éŸ³é¢‘å·²æ˜¯16kHzå•å£°é“æ ¼å¼: {output_wav}")
            return
        else:
            print(f"âš ï¸ éŸ³é¢‘æ ¼å¼ä¸æ­£ç¡® (é‡‡æ ·ç‡: {sample_rate}Hz, å£°é“: {channels}), éœ€è¦è½¬æ¢")
            need_convert = True
    else:
        need_convert = True

    if need_convert:
        print(f"ğŸ”„ è½¬æ¢éŸ³é¢‘ä¸º16kHzå•å£°é“WAV: {input_file} -> {output_wav}")
        # ç»Ÿä¸€çš„è½¬æ¢å‘½ä»¤ï¼š16kHz, å•å£°é“, PCM 16ä½
        cmd = f"ffmpeg -i '{input_file}' -vn -acodec pcm_s16le -ar 16000 -ac 1 -y '{output_wav}' -v error"
        execute(cmd)

        # éªŒè¯è½¬æ¢ç»“æœ
        sample_rate, channels = get_audio_info(output_wav)
        if sample_rate == 16000 and channels == 1:
            print(f"âœ… éŸ³é¢‘è½¬æ¢æˆåŠŸ: 16kHzå•å£°é“")
        else:
            print(f"âŒ éŸ³é¢‘è½¬æ¢å¯èƒ½å¤±è´¥: é‡‡æ ·ç‡={sample_rate}Hz, å£°é“={channels}")


def extract_audio(input_video: Path, output_wav: Path):
    """ä»è§†é¢‘æ–‡ä»¶æå–16kHzå•å£°é“éŸ³é¢‘"""
    ensure_16khz_mono_wav(input_video, output_wav)


def cost(start, cmd: str = "", prefix=""):
    end = datetime.datetime.now()
    elapsed = (end - start).total_seconds()
    mins = elapsed // 60
    seconds = elapsed % 60
    print(
        f"{prefix}{end.hour:02d}:{end.minute:02d}:{end.second:02d} {cmd}ç”¨æ—¶{mins}åˆ†{seconds:.0f}ç§’"
    )


def detect_silence_boundaries(audio_segment, sample_rate=16000, silence_threshold=0.01, min_silence_duration=0.3):
    """
    æ£€æµ‹éŸ³é¢‘ç‰‡æ®µä¸­çš„é™éŸ³è¾¹ç•Œï¼Œç”¨äºè¿›ä¸€æ­¥åˆ†å‰²é•¿ç‰‡æ®µ

    Args:
        audio_segment: éŸ³é¢‘æ•°æ®
        sample_rate: é‡‡æ ·ç‡
        silence_threshold: é™éŸ³é˜ˆå€¼
        min_silence_duration: æœ€å°é™éŸ³æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        list: é™éŸ³è¾¹ç•Œçš„æ—¶é—´ç‚¹ï¼ˆç›¸å¯¹äºç‰‡æ®µå¼€å§‹çš„ç§’æ•°ï¼‰
    """
    import numpy as np

    # è®¡ç®—éŸ³é¢‘èƒ½é‡
    window_size = int(0.1 * sample_rate)  # 100msçª—å£
    energy = []

    for i in range(0, len(audio_segment) - window_size, window_size // 2):
        window = audio_segment[i:i + window_size]
        energy.append(np.mean(window ** 2))

    energy = np.array(energy)

    # æ£€æµ‹é™éŸ³åŒºåŸŸ
    silence_mask = energy < silence_threshold

    # æ‰¾åˆ°é™éŸ³åŒºåŸŸçš„è¾¹ç•Œ
    boundaries = []
    in_silence = False
    silence_start = 0

    for i, is_silent in enumerate(silence_mask):
        time_pos = i * (window_size // 2) / sample_rate

        if is_silent and not in_silence:
            # é™éŸ³å¼€å§‹
            silence_start = time_pos
            in_silence = True
        elif not is_silent and in_silence:
            # é™éŸ³ç»“æŸ
            silence_duration = time_pos - silence_start
            if silence_duration >= min_silence_duration:
                # åœ¨é™éŸ³ä¸­ç‚¹æ·»åŠ è¾¹ç•Œ
                boundary_time = silence_start + silence_duration / 2
                boundaries.append(boundary_time)
            in_silence = False

    return boundaries


def filter_prompt_artifacts(segments, prompts_to_remove=None):
    """
    è¿‡æ»¤æ‰promptæ³„éœ²å’Œé‡å¤å†…å®¹

    Args:
        segments: åŸå§‹ç‰‡æ®µåˆ—è¡¨
        prompts_to_remove: è¦ç§»é™¤çš„promptåˆ—è¡¨

    Returns:
        list: è¿‡æ»¤åçš„ç‰‡æ®µåˆ—è¡¨
    """
    if prompts_to_remove is None:
        prompts_to_remove = [
            "è¯·è¾“å‡ºç®€ä½“ä¸­æ–‡ã€‚",
            "è¯·è¾“å‡ºç®€ä½“ä¸­æ–‡",
            "å¤§å®¶å¥½ï¼Œæˆ‘ä»¬å¼€å§‹ä¸Šè¯¾äº†ã€‚",
            "å¤§å®¶å¥½ï¼Œæˆ‘ä»¬å¼€å§‹ä¸Šè¯¾äº†",
            "è¯·è¾“å‡ºç®€ä½“ä¸­æ–‡ã€‚è¯·è¾“å‡ºç®€ä½“ä¸­æ–‡ã€‚",
        ]

    filtered_segments = []

    for segment in segments:
        text = segment["text"].strip()

        # æ£€æŸ¥æ˜¯å¦æ˜¯promptå†…å®¹
        is_prompt = False
        for prompt in prompts_to_remove:
            if text == prompt or text.replace("ã€‚", "") == prompt.replace("ã€‚", ""):
                is_prompt = True
                break

        # æ£€æŸ¥æ˜¯å¦æ˜¯é‡å¤çš„çŸ­å†…å®¹
        if len(text) < 10 and text.count("ã€‚") >= 2:
            is_prompt = True

        # æ£€æŸ¥æ˜¯å¦æ˜¯ç©ºç™½æˆ–æ— æ„ä¹‰å†…å®¹
        if not text or len(text.strip()) < 2:
            is_prompt = True

        if not is_prompt:
            filtered_segments.append(segment)

    return filtered_segments


def split_long_segments_by_punctuation(segments, max_duration=4.0):
    """
    åŸºäºæ ‡ç‚¹ç¬¦å·å’Œæ—¶é•¿å°†è¿‡é•¿çš„ç‰‡æ®µåˆ†å‰²

    Args:
        segments: åŸå§‹ç‰‡æ®µåˆ—è¡¨
        max_duration: æœ€å¤§ç‰‡æ®µæŒç»­æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        list: åˆ†å‰²åçš„ç‰‡æ®µåˆ—è¡¨
    """
    new_segments = []

    for segment in segments:
        duration = segment["end"] - segment["start"]

        if duration <= max_duration:
            # ç‰‡æ®µä¸é•¿ï¼Œç›´æ¥ä¿ç•™
            new_segments.append(segment)
            continue

        text = segment["text"].strip()

        # å¯»æ‰¾åˆé€‚çš„åˆ†å‰²ç‚¹ï¼ˆå¥å·ã€é—®å·ã€æ„Ÿå¹å·ã€é€—å·ï¼‰
        split_chars = ['ã€‚', 'ï¼Ÿ', 'ï¼', 'ï¼Œ', 'ã€']
        split_positions = []

        for i, char in enumerate(text):
            if char in split_chars:
                split_positions.append(i + 1)  # åŒ…å«æ ‡ç‚¹ç¬¦å·

        if not split_positions:
            # æ²¡æœ‰æ ‡ç‚¹ç¬¦å·ï¼ŒæŒ‰å­—æ•°å¹³å‡åˆ†å‰²
            mid_point = len(text) // 2
            split_positions = [mid_point]

        # é€‰æ‹©æœ€ä½³åˆ†å‰²ç‚¹ï¼ˆå°½é‡åœ¨ä¸­é—´ï¼‰
        target_pos = len(text) // 2
        best_pos = min(split_positions, key=lambda x: abs(x - target_pos))

        # åˆ†å‰²æ–‡æœ¬
        text1 = text[:best_pos].strip()
        text2 = text[best_pos:].strip()

        if text1 and text2:
            # æŒ‰æ—¶é—´æ¯”ä¾‹åˆ†é…
            time_ratio = len(text1) / len(text)
            split_time = segment["start"] + duration * time_ratio

            new_segments.append({
                "start": segment["start"],
                "end": split_time,
                "text": text1
            })

            new_segments.append({
                "start": split_time,
                "end": segment["end"],
                "text": text2
            })
        else:
            # åˆ†å‰²å¤±è´¥ï¼Œä¿ç•™åŸç‰‡æ®µ
            new_segments.append(segment)

    return new_segments


def speechbrain_speaker_diarization(segments, audio, audio_file_path):
    """
    ä½¿ç”¨SpeechBrainè¿›è¡Œè¯´è¯äººåˆ†ç¦»ï¼Œæ”¯æŒé•¿ç‰‡æ®µçš„æ™ºèƒ½åˆ†å‰²

    Args:
        segments: WhisperXè½¬å½•çš„ç‰‡æ®µ
        audio: éŸ³é¢‘æ•°æ®
        audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

    Returns:
        pysubs2.SSAFile: å¸¦è¯´è¯äººæ ‡ç­¾çš„å­—å¹•å¯¹è±¡
    """
    print("ğŸ­ ä½¿ç”¨SpeechBrainè¿›è¡Œè¯´è¯äººåˆ†ç¦»...")

    try:
        from speechbrain.inference import SpeakerRecognition
        import torch
        import numpy as np

        # é¦–å…ˆè¿‡æ»¤promptæ³„éœ²å’Œæ— æ•ˆå†…å®¹
        print("ğŸ§¹ è¿‡æ»¤promptæ³„éœ²å’Œæ— æ•ˆå†…å®¹...")
        original_count = len(segments)
        segments = filter_prompt_artifacts(segments)
        filtered_count = len(segments)
        if original_count > filtered_count:
            print(f"å·²è¿‡æ»¤ {original_count - filtered_count} ä¸ªæ— æ•ˆç‰‡æ®µ")

        # ç„¶ååˆ†å‰²è¿‡é•¿çš„ç‰‡æ®µ
        print("ğŸ”ª åˆ†å‰²è¿‡é•¿çš„éŸ³é¢‘ç‰‡æ®µ...")
        segments = split_long_segments_by_punctuation(segments, max_duration=4.0)
        print(f"æœ€ç»ˆå…±æœ‰ {len(segments)} ä¸ªæœ‰æ•ˆç‰‡æ®µ")

        # åŠ è½½è¯´è¯äººè¯†åˆ«æ¨¡å‹
        print("åŠ è½½SpeechBrainè¯´è¯äººè¯†åˆ«æ¨¡å‹...")
        verification = SpeakerRecognition.from_hparams(
            source='speechbrain/spkrec-ecapa-voxceleb',
            savedir='tmp/spkrec-ecapa-voxceleb'
        )

        # ä¸ºæ¯ä¸ªç‰‡æ®µæå–è¯´è¯äººç‰¹å¾
        print("æå–è¯´è¯äººç‰¹å¾...")
        speaker_embeddings = []
        valid_segments = []

        for i, segment in enumerate(segments):
            start_time = segment["start"]
            end_time = segment["end"]

            # æå–éŸ³é¢‘ç‰‡æ®µ
            start_sample = int(start_time * 16000)  # å‡è®¾16kHzé‡‡æ ·ç‡
            end_sample = int(end_time * 16000)

            if end_sample > len(audio):
                end_sample = len(audio)
            if start_sample >= end_sample:
                continue

            audio_segment = audio[start_sample:end_sample]

            # ç¡®ä¿éŸ³é¢‘ç‰‡æ®µè¶³å¤Ÿé•¿ï¼ˆè‡³å°‘0.5ç§’ï¼‰
            if len(audio_segment) < 8000:  # 0.5ç§’ * 16000Hz
                continue

            # è½¬æ¢ä¸ºtorch tensor
            audio_tensor = torch.FloatTensor(audio_segment).unsqueeze(0)

            # æå–è¯´è¯äººåµŒå…¥
            try:
                embedding = verification.encode_batch(audio_tensor)
                speaker_embeddings.append(embedding.squeeze().cpu().numpy())
                valid_segments.append(segment)
            except Exception as e:
                print(f"âš ï¸ ç‰‡æ®µ {i} ç‰¹å¾æå–å¤±è´¥: {e}")
                continue

        if len(speaker_embeddings) < 2:
            print("âš ï¸ æœ‰æ•ˆéŸ³é¢‘ç‰‡æ®µå¤ªå°‘ï¼Œæ— æ³•è¿›è¡Œè¯´è¯äººåˆ†ç¦»")
            # å›é€€åˆ°æ™®é€šå­—å¹•
            return pysubs2.load_from_whisper(segments)

        # ä½¿ç”¨èšç±»ç®—æ³•åˆ†ç¦»è¯´è¯äºº
        print("æ‰§è¡Œè¯´è¯äººèšç±»...")
        from sklearn.cluster import AgglomerativeClustering

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        embeddings_array = np.array(speaker_embeddings)

        # è‡ªåŠ¨ç¡®å®šè¯´è¯äººæ•°é‡ï¼ˆ2-4ä¸ªï¼‰
        best_score = -1
        best_labels = None
        best_n_speakers = 2

        for n_speakers in range(2, min(5, len(embeddings_array) + 1)):
            clustering = AgglomerativeClustering(
                n_clusters=n_speakers,
                linkage='ward'
            )
            labels = clustering.fit_predict(embeddings_array)

            # è®¡ç®—è½®å»“ç³»æ•°ä½œä¸ºèšç±»è´¨é‡è¯„ä¼°
            from sklearn.metrics import silhouette_score
            if len(set(labels)) > 1:
                score = silhouette_score(embeddings_array, labels)
                if score > best_score:
                    best_score = score
                    best_labels = labels
                    best_n_speakers = n_speakers

        if best_labels is None:
            print("âš ï¸ èšç±»å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†é…")
            best_labels = [i % 2 for i in range(len(valid_segments))]
            best_n_speakers = 2

        print(f"âœ… æ£€æµ‹åˆ° {best_n_speakers} ä¸ªè¯´è¯äººï¼Œèšç±»è´¨é‡åˆ†æ•°: {best_score:.3f}")

        # ä¸ºç‰‡æ®µåˆ†é…è¯´è¯äººæ ‡ç­¾
        speaker_names = {
            0: 'è¯´è¯äººA',
            1: 'è¯´è¯äººB',
            2: 'è¯´è¯äººC',
            3: 'è¯´è¯äººD',
            4: 'è¯´è¯äººE',
            5: 'è¯´è¯äººF'
        }

        # ç»Ÿè®¡æ¯ä¸ªè¯´è¯äººçš„ç‰‡æ®µæ•°
        speaker_stats = {}
        for label in best_labels:
            speaker_name = speaker_names.get(label, f'è¯´è¯äºº{label}')
            speaker_stats[speaker_name] = speaker_stats.get(speaker_name, 0) + 1

        print(f"\nğŸ­ è¯´è¯äººåˆ†ç¦»ç»Ÿè®¡:")
        for speaker, count in speaker_stats.items():
            print(f"   {speaker}: {count} ä¸ªç‰‡æ®µ")

        # åˆ›å»ºå¸¦è¯´è¯äººæ ‡ç­¾çš„å­—å¹•
        subs = pysubs2.SSAFile()

        # å¤„ç†æ‰€æœ‰åŸå§‹ç‰‡æ®µï¼Œä¸ºæœ‰æ•ˆç‰‡æ®µåˆ†é…è¯´è¯äººæ ‡ç­¾
        valid_idx = 0
        for segment in segments:
            start_ms = int(segment["start"] * 1000)
            end_ms = int(segment["end"] * 1000)
            text = segment["text"].strip()

            if not text:
                continue

            # æ£€æŸ¥è¿™ä¸ªç‰‡æ®µæ˜¯å¦åœ¨æœ‰æ•ˆç‰‡æ®µä¸­
            if (valid_idx < len(valid_segments) and
                abs(segment["start"] - valid_segments[valid_idx]["start"]) < 0.1):
                # è¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆç‰‡æ®µï¼Œæœ‰è¯´è¯äººæ ‡ç­¾
                speaker_label = best_labels[valid_idx]
                speaker_name = speaker_names.get(speaker_label, f'è¯´è¯äºº{speaker_label}')
                labeled_text = f"[{speaker_name}] {text}"
                valid_idx += 1
            else:
                # è¿™æ˜¯ä¸€ä¸ªæ— æ•ˆç‰‡æ®µï¼Œä½¿ç”¨é»˜è®¤æ ‡ç­¾
                labeled_text = f"[è¯´è¯äºº] {text}"

            event = pysubs2.SSAEvent(
                start=start_ms,
                end=end_ms,
                text=labeled_text
            )
            subs.events.append(event)

        return subs

    except ImportError as e:
        print(f"âŒ SpeechBrainå¯¼å…¥å¤±è´¥: {e}")
        raise
    except Exception as e:
        print(f"âŒ SpeechBrainè¯´è¯äººåˆ†ç¦»å¤±è´¥: {e}")
        raise


def is_audio_file(file_path: Path) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºéŸ³é¢‘æ–‡ä»¶"""
    audio_extensions = {'.wav', '.mp3', '.m4a', '.flac', '.aac', '.ogg', '.wma'}
    return file_path.suffix.lower() in audio_extensions


def is_video_file(file_path: Path) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºè§†é¢‘æ–‡ä»¶"""
    video_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv'}
    return file_path.suffix.lower() in video_extensions


def transcript(input_file: Path, output_dir: Path = None, dry_run=False, enable_diarization=True):
    """
    å°†è§†é¢‘æˆ–éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºå­—å¹•æ–‡ä»¶ï¼Œè‡ªåŠ¨ç”Ÿæˆä¸¤ä¸ªç‰ˆæœ¬ï¼š
    1. SRTæ–‡ä»¶ï¼ˆä¸å¸¦å¯¹è¯äººæ ‡è¯†ï¼‰
    2. æ™®é€šæ–‡æœ¬æ–‡ä»¶ï¼ˆæ¯è¡Œå°è¯å‰å¸¦æœ‰å¯¹è¯äººæ ‡è¯†ï¼‰

    Args:
        input_file: è¾“å…¥è§†é¢‘æˆ–éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™å°†srtæ–‡ä»¶ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•
        dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼
        enable_diarization: æ˜¯å¦å¯ç”¨è¯´è¯äººåˆ†ç¦»åŠŸèƒ½ï¼ˆé»˜è®¤ä¸ºTrueï¼‰
    """
    input_file = Path(input_file)

    # éªŒè¯è¾“å…¥æ–‡ä»¶å­˜åœ¨
    if not input_file.exists():
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")

    # éªŒè¯æ–‡ä»¶æ ¼å¼
    is_audio = is_audio_file(input_file)
    is_video = is_video_file(input_file)

    if not (is_audio or is_video):
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {input_file.suffix}ã€‚æ”¯æŒçš„æ ¼å¼ï¼šè§†é¢‘(.mp4, .avi, .mov, .mkv, .flv, .wmv)ï¼ŒéŸ³é¢‘(.wav, .mp3, .m4a, .flac, .aac, .ogg, .wma)")

    # ä½¿ç”¨æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºé¡¹ç›®åç§°
    name = input_file.stem

    # è®¾ç½®å·¥ä½œç›®å½•ï¼ˆç”¨äºä¸´æ—¶æ–‡ä»¶ï¼‰
    working_dir = Path("/tmp/transcript") / name

    # è®¾ç½®æœ€ç»ˆsrtæ–‡ä»¶çš„è¾“å‡ºç›®å½•
    if output_dir is None:
        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆtranscript.pyæ‰€åœ¨ç›®å½•çš„ä¸Šçº§ç›®å½•ï¼‰
        script_dir = Path(__file__).parent
        project_root = script_dir.parent
        final_output_dir = project_root
    else:
        final_output_dir = Path(output_dir)

    working_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜å·¥ä½œæ—¥å¿—
    log_file = "/tmp/transcript.log"
    with open(log_file, "w", encoding="utf-8") as f:
        json.dump({
            "working_dir": str(working_dir),
            "name": name,
            "raw_file": str(input_file),
            "file_type": "audio" if is_audio else "video",
            "timestamp": datetime.datetime.now().isoformat()
            }, f, indent=2)

    # å¤åˆ¶æ–‡ä»¶åˆ°å·¥ä½œç›®å½•
    media_file = working_dir / input_file.name
    if not media_file.exists():
        print(f"å¤åˆ¶{'éŸ³é¢‘' if is_audio else 'è§†é¢‘'}æ–‡ä»¶åˆ°å·¥ä½œç›®å½•: {input_file} -> {media_file}")
        shutil.copy(input_file, media_file)

    # è®¾ç½®ä¸´æ—¶srtæ–‡ä»¶è·¯å¾„ï¼ˆåœ¨å·¥ä½œç›®å½•ä¸­ï¼‰
    temp_srt = working_dir / f"{name}.srt"

    # è®¾ç½®æœ€ç»ˆsrtæ–‡ä»¶è·¯å¾„ï¼ˆåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸­ï¼‰
    final_srt = final_output_dir / f"{name}.srt"

    print(f"å¼€å§‹è½¬æ¢{'éŸ³é¢‘' if is_audio else 'è§†é¢‘'}ä¸ºå­—å¹•: {input_file} -> {final_srt}")

    # å¤„ç†éŸ³é¢‘
    start = datetime.datetime.now()
    print(f"{start.hour:02d}:{start.minute:02d}:{start.second:02d}: å¼€å§‹å¤„ç†")

    # åˆ›å»ºä¸“ç”¨çš„16kHzå•å£°é“éŸ³é¢‘æ–‡ä»¶ç”¨äºè½¬å½•ï¼ˆä¸è¦†ç›–åŸæ–‡ä»¶ï¼‰
    transcription_wav = media_file.parent / f"{media_file.stem}_transcription.wav"

    # ç»Ÿä¸€å¤„ç†ï¼šæ— è®ºéŸ³é¢‘è¿˜æ˜¯è§†é¢‘ï¼Œéƒ½è½¬æ¢ä¸º16kHzå•å£°é“ç”¨äºè½¬å½•
    file_type = "éŸ³é¢‘" if is_audio else "è§†é¢‘"
    print(f"ğŸ“ ä»{file_type}åˆ›å»º16kHzå•å£°é“éŸ³é¢‘ç”¨äºè½¬å½•: {transcription_wav.name}")
    ensure_16khz_mono_wav(media_file, transcription_wav, force_convert=True)

    # ç”Ÿæˆå­—å¹•åˆ°ä¸´æ—¶ä½ç½®
    print("ç”Ÿæˆå­—å¹•...")

    # æ£€æŸ¥whisper.cppæ˜¯å¦å¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨whisperx
    whisper_cpp_available = cpp_path.exists() and cpp_model.exists()

    if enable_diarization:
        # è¯´è¯äººåˆ†ç¦»å¿…é¡»ä½¿ç”¨whisperxï¼Œä¸ä½¿ç”¨whisper.cpp
        print("ğŸ­ è¯´è¯äººåˆ†ç¦»åŠŸèƒ½éœ€è¦ä½¿ç”¨whisperx...")
        transcriptx_with_diarization(transcription_wav, temp_srt, prompt)
    elif whisper_cpp_available and not dry_run:
        # ä½¿ç”¨whisper.cppè¿›è¡Œçº¯è½¬å½•ï¼ˆæ— è¯´è¯äººåˆ†ç¦»ï¼‰
        try:
            print("ğŸš€ ä½¿ç”¨whisper.cppè¿›è¡Œè½¬å½•...")
            transcript_cpp(transcription_wav, temp_srt, prompt, dry_run)
        except Exception as e:
            print(f"âš ï¸ whisper.cppè½¬å½•å¤±è´¥: {e}")
            print("å›é€€åˆ°whisperxè½¬å½•...")
            transcriptx(transcription_wav, temp_srt, prompt)
    else:
        # ä½¿ç”¨whisperxè¿›è¡Œè½¬å½•
        if not whisper_cpp_available:
            print("âš ï¸ whisper.cppä¸å¯ç”¨ï¼Œä½¿ç”¨whisperxè½¬å½•...")
        transcriptx(transcription_wav, temp_srt, prompt)

    # æ£€æŸ¥å­—å¹•æ–‡ä»¶æ˜¯å¦ç”ŸæˆæˆåŠŸ
    if not temp_srt.exists():
        raise FileNotFoundError(f"å­—å¹•ç”Ÿæˆå¤±è´¥ï¼Œæ–‡ä»¶ä¸å­˜åœ¨: {temp_srt}")

    print(f"âœ… å­—å¹•æ–‡ä»¶ç”ŸæˆæˆåŠŸ: {temp_srt}")

    # åº”ç”¨è‡ªå®šä¹‰è¯å…¸çº é”™
    print("åº”ç”¨è¯å…¸çº é”™...")
    sub(temp_srt)

    # ç”Ÿæˆä¸¤ä¸ªç‰ˆæœ¬çš„æ–‡ä»¶
    print("ç”Ÿæˆä¸¤ä¸ªç‰ˆæœ¬çš„è½¬å½•æ–‡ä»¶...")

    # 1. ç”Ÿæˆå¹²å‡€çš„SRTæ–‡ä»¶ï¼ˆä¸å¸¦å¯¹è¯äººæ ‡è¯†ï¼‰
    final_clean_srt = final_output_dir / f"{name}.srt"
    create_clean_srt_file(temp_srt, final_clean_srt)

    # 2. ç”Ÿæˆå¸¦è¯´è¯äººæ ‡è¯†çš„æ–‡æœ¬æ–‡ä»¶
    final_speaker_txt = final_output_dir / f"{name}-speakers.txt"
    create_speaker_text_file(temp_srt, final_speaker_txt)

    cost(start, prefix="å­—å¹•ç”Ÿæˆå®Œæˆ ")
    print(f"\n=== è½¬å½•æ–‡ä»¶ç”Ÿæˆå®Œæˆ ===")
    print(f"âœ… SRTå­—å¹•æ–‡ä»¶ï¼ˆæ— è¯´è¯äººæ ‡è¯†ï¼‰: {final_clean_srt}")
    print(f"âœ… æ–‡æœ¬æ–‡ä»¶ï¼ˆå«è¯´è¯äººæ ‡è¯†ï¼‰: {final_speaker_txt}")
    print(f"å·¥ä½œç›®å½•: {working_dir}")
    print("\nğŸ’¡ æç¤º:")
    print(f"   - ç¼–è¾‘SRTæ–‡ä»¶è¿›è¡Œå­—å¹•æ ¡å¯¹")
    print(f"   - æ–‡æœ¬æ–‡ä»¶å¯ç”¨äºå…¶ä»–ç”¨é€”")
    print(f"   - ç¼–è¾‘å®Œæˆåè¿è¡Œ 'transcript resume' ç»§ç»­å¤„ç†")

    return final_clean_srt, final_speaker_txt


def probe_duration(video):
    """returns duration of a video in seconds"""
    cmd = [
        "ffprobe",
        "-v",
        "error",
        "-show_entries",
        "format=duration",
        "-of",
        "default=noprint_wrappers=1:nokey=1",
        video,
    ]
    duration = float(subprocess.check_output(cmd).strip())
    return duration


def cut_video(input_video: Path, to_del: List[Tuple[int, int, int]], out_dir: Path):
    """æ ¹æ®sliceså‰ªå»è§†é¢‘ç‰‡æ®µ

    ffmpeg -hide_banner -ss '60.08300' -i '/private/tmp/fa.mp4' -t '178.00000' -avoid_negative_ts make_zero -map '0:0' '-c:0' copy -map '0:1' '-c:1' copy -map_metadata 0 -movflags '+faststart' -default_mode infer_no_subs -ignore_unknown -f mp4 -y '/private/tmp/fa-00.01.00.083-00.03.58.083-seg2.mp4'
    """
    duration = probe_duration(input_video) * 1000

    edges = [item for sublist in to_del for item in sublist[1:]]
    edges.insert(0, 0)
    edges.append(int(duration))

    if edges[:2] == [0, 0]:
        edges = edges[2:]

    slices = [(edges[i], edges[i + 1]) for i in range(0, len(edges) - 1, 2)]

    tmp_files = []

    for i, (start, end) in enumerate(slices):
        start_time = _ms_to_hms(start)
        end_time = _ms_to_hms(end)
        start_secs = f"{start/1000:.3f}"
        dur = f"'{(end - start)/1000:.3f}'"

        tmp_file = os.path.join(out_dir, f"{i}-{start_time}-{end_time}.mp4")
        tmp_files.append(tmp_file)
        cmd = f"ffmpeg -hide_banner -ss '{start_secs}' -i '{input_video}' -t '{dur}' -c copy -map 0:v -map 0:a -map_metadata 0 -movflags '+faststart' -f mp4 -video_track_timescale 600 -y -v error '{tmp_file}'"

        execute(cmd, supress_log=True)

    list_file = os.path.join(out_dir, "list.text")
    with open(list_file, "w", encoding="utf-8") as f:
        for video in tmp_files:
            f.write(f"file '{video}'\n")

    return list_file


def cut_audio(input_audio: Path, to_del: List[Tuple[int, int, int]], out_dir: Path):
    """æ ¹æ®sliceså‰ªå»éŸ³é¢‘ç‰‡æ®µ

    ç±»ä¼¼cut_videoä½†å¤„ç†éŸ³é¢‘æ–‡ä»¶
    """
    duration = probe_duration(input_audio) * 1000

    edges = [item for sublist in to_del for item in sublist[1:]]
    edges.insert(0, 0)
    edges.append(int(duration))

    if edges[:2] == [0, 0]:
        edges = edges[2:]

    slices = [(edges[i], edges[i + 1]) for i in range(0, len(edges) - 1, 2)]

    tmp_files = []

    for i, (start, end) in enumerate(slices):
        start_time = _ms_to_hms(start)
        end_time = _ms_to_hms(end)
        start_secs = f"{start/1000:.3f}"
        dur = f"'{(end - start)/1000:.3f}'"

        # ä¿æŒåŸéŸ³é¢‘æ ¼å¼
        audio_ext = input_audio.suffix
        tmp_file = os.path.join(out_dir, f"{i}-{start_time}-{end_time}{audio_ext}")
        tmp_files.append(tmp_file)

        # éŸ³é¢‘å‰ªè¾‘å‘½ä»¤ï¼Œä¸éœ€è¦è§†é¢‘ç›¸å…³å‚æ•°
        cmd = f"ffmpeg -hide_banner -ss '{start_secs}' -i '{input_audio}' -t '{dur}' -c copy -y -v error '{tmp_file}'"

        execute(cmd, supress_log=True)

    list_file = os.path.join(out_dir, "list.text")
    with open(list_file, "w", encoding="utf-8") as f:
        for audio in tmp_files:
            f.write(f"file '{audio}'\n")

    return list_file


def sub(srt_file: Path):
    """è¾“å…¥å­—å¹•æ–‡ä»¶ï¼Œé€šè¿‡è‡ªå®šä¹‰è¯å…¸ï¼Œå…ˆç²—ç­›ä¸€æ¬¡ã€‚åœ¨transcriptä¹‹åè¢«è‡ªåŠ¨è°ƒç”¨ã€‚

    Args:
        srt_file (str): _description_
    """
    try:
        subs = pysubs2.load(str(srt_file))
    except Exception as e:
        print(f"âš ï¸ åŠ è½½å­—å¹•æ–‡ä»¶å¤±è´¥: {e}")
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºç©ºæˆ–æ ¼å¼ä¸æ­£ç¡®
        if srt_file.exists():
            with open(srt_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            if not content:
                print("å­—å¹•æ–‡ä»¶ä¸ºç©ºï¼Œåˆ›å»ºç©ºçš„å­—å¹•å¯¹è±¡")
                subs = pysubs2.SSAFile()
                # ä¿å­˜ç©ºå­—å¹•æ–‡ä»¶ä»¥ç¡®ä¿æ ¼å¼æ­£ç¡®
                subs.save(str(srt_file))
                return  # ç©ºæ–‡ä»¶æ— éœ€è¿›ä¸€æ­¥å¤„ç†
            else:
                print(f"å­—å¹•æ–‡ä»¶å†…å®¹: {content[:200]}...")
                raise
        else:
            print("å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºç©ºçš„å­—å¹•å¯¹è±¡")
            subs = pysubs2.SSAFile()
            subs.save(str(srt_file))
            return

    replace_map, warning_words = init_jieba()
    warnings_found = []

    for i, event in enumerate(subs.events):
        text = event.text

        # å…ˆåº”ç”¨æ›¿æ¢è¯å…¸
        replaced = "".join([replace_map.get(x, x) for x in jieba.cut(text)])
        if event.text != replaced:
            print(f"ğŸ”„ è¯å…¸æ›¿æ¢: {event.text} -> {replaced}")
        event.text = replaced

        # å¯¹æ›¿æ¢åçš„å†…å®¹æ£€æŸ¥è­¦å‘Šè¯
        for word in warning_words:
            if word in event.text:
                warnings_found.append({
                    'word': word,
                    'text': event.text,
                    'index': i + 1
                })

    subs.save(str(srt_file))

    # é›†ä¸­è¾“å‡ºè­¦å‘Šä¿¡æ¯
    if warnings_found:
        print(f"\n{'='*60}")
        print(f"âš ï¸  å‘ç° {len(warnings_found)} ä¸ªéœ€è¦äººå·¥å¤æ£€çš„å­—å¹•")
        print(f"{'='*60}")
        for warning in warnings_found:
            print(f"å­—å¹• {warning['index']}: å‘ç°è¯æ±‡ '{warning['word']}'")
            print(f"å†…å®¹: {warning['text']}")
            print("-" * 40)
        print(f"è¯·æ£€æŸ¥ä»¥ä¸Šå­—å¹•å†…å®¹æ˜¯å¦éœ€è¦æ‰‹åŠ¨è°ƒæ•´")
        print(f"{'='*60}\n")


def cut():
    """å­—å¹•ç¼–è¾‘ä¹‹åï¼Œè¿›è¡Œè§†é¢‘åˆ‡åˆ†ã€åˆå¹¶ã€å‹å­—å¹•ã€å‹ç¼©åŠæ‹·è´

    1. å‰ªæ‰è¯­åŠ©ï¼ˆå•è¡Œçš„å¥½ï¼Œå‘ƒç­‰ï¼‰
    2. å‰ªæ‰å­—å¹•ä¸­ä»¥[del]å¼€å¤´çš„event
    3. æ ¹æ®è‡ªå®šä¹‰è¯å…¸å®Œæˆæ›¿æ¢

    Args:
        working_dir: å·¥ä½œç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä»/tmp/transcript.logæ–‡ä»¶è¯»å–
    """
    # è¯»å–å·¥ä½œæ—¥å¿—
    log_file = Path("/tmp/transcript.log")
    if not log_file.exists():
        raise FileNotFoundError("æ‰¾ä¸åˆ°å·¥ä½œæ—¥å¿—æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œtranscriptå‘½ä»¤")

    with open(log_file, "r", encoding="utf-8") as f:
        log = json.load(f)
        name = log["name"]
        # å…¼å®¹æ—§ç‰ˆæœ¬æ—¥å¿—æ ¼å¼
        if "raw_file" in log:
            media_file = Path(log["raw_file"])
            file_type = log.get("file_type", "video")  # é»˜è®¤ä¸ºè§†é¢‘
        else:
            # å…¼å®¹æ—§ç‰ˆæœ¬
            media_file = Path(log["raw_video"])
            file_type = "video"
        parent = Path(log["working_dir"])

    # æŸ¥æ‰¾å­—å¹•æ–‡ä»¶ - ä¼˜å…ˆæŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    srt_file = project_root / f"{name}.srt"

    if not srt_file.exists():
        # å°è¯•åœ¨å·¥ä½œç›®å½•æŸ¥æ‰¾
        srt_file = parent / f"{name}.srt"
        if not srt_file.exists():
            # å°è¯•åœ¨å½“å‰ç›®å½•æŸ¥æ‰¾
            srt_file = Path(f"./{name}.srt").resolve()
            if not srt_file.exists():
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°å­—å¹•æ–‡ä»¶: {srt_file}")

    if not media_file.exists():
        # å°è¯•åœ¨å·¥ä½œç›®å½•æŸ¥æ‰¾åª’ä½“æ–‡ä»¶
        media_in_workspace = parent / Path(media_file).name
        if media_in_workspace.exists():
            media_file = media_in_workspace
        else:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°{'éŸ³é¢‘' if file_type == 'audio' else 'è§†é¢‘'}æ–‡ä»¶: {media_file}")

    print(f"å¤„ç†å­—å¹•æ–‡ä»¶: {srt_file}")
    print(f"å¤„ç†{'éŸ³é¢‘' if file_type == 'audio' else 'è§†é¢‘'}æ–‡ä»¶: {media_file}")

    workspace = parent / "cut"
    workspace.mkdir(exist_ok=True)

    out_srt = parent / "cut.srt"

    replace_map, warning_words = init_jieba()
    markers = "å¥½å‘ƒæ©å—¯"

    to_del = []

    subs = pysubs2.load(str(srt_file))
    keep_subs = pysubs2.SSAFile()
    cum_lag: int = 0

    def remove_event(to_del: list, i: int, event: Any) -> int:
        # é‡åˆ°è¿ç»­åˆ é™¤ï¼Œè¦è¿›è¡Œåˆå¹¶
        if len(to_del) > 0 and to_del[-1][0] == i - 1:
            item = to_del[-1]
            item[0] = i
            lag = event.end - item[2]
            item[2] = event.end
            return lag
        else:
            to_del.append([i, event.start, event.end])
            return event.duration

    deleted_count = 0
    for i, event in enumerate(subs.events):
        text = event.text
        if len(text) == 1 and text in markers:
            cum_lag += remove_event(to_del, i, event)
            deleted_count += 1
            print(f"åˆ é™¤è¯­åŠ©è¯: {event.text}")
            continue

        if text.startswith("[del]") or text.startswith("[DEL]"):
            cum_lag += remove_event(to_del, i, event)
            deleted_count += 1
            print(f"åˆ é™¤æ ‡è®°å­—å¹•: {event.text}")
            continue

        # å…ˆåº”ç”¨æ›¿æ¢è¯å…¸
        replaced = "".join([replace_map.get(x, x) for x in jieba.cut(text)])
        if event.text != replaced:
            print(f"ğŸ”„ è¯å…¸æ›¿æ¢: {event.text} -> {replaced}")

        event.text = replaced

        # å¯¹æ›¿æ¢åçš„å†…å®¹æ£€æŸ¥è­¦å‘Šè¯ï¼ˆåœ¨cuté˜¶æ®µä¸é›†ä¸­è¾“å‡ºï¼Œå› ä¸ºå¯èƒ½æœ‰åˆ é™¤ï¼‰
        for word in warning_words:
            if word in event.text:
                print(f"âš ï¸  è­¦å‘Š: å­—å¹•ä¸­å‘ç°éœ€è¦äººå·¥å¤æ£€çš„è¯æ±‡ '{word}': {event.text}")
        event.start -= cum_lag
        event.end -= cum_lag
        keep_subs.events.append(event)

    keep_subs.save(str(out_srt))
    print(f"åˆ é™¤äº† {deleted_count} ä¸ªå­—å¹•ç‰‡æ®µ")
    print(f"ä¿ç•™äº† {len(keep_subs.events)} ä¸ªå­—å¹•ç‰‡æ®µ")

    # åˆ‡åˆ†åª’ä½“æ–‡ä»¶
    if to_del:
        if file_type == "audio":
            print("å¼€å§‹åˆ‡åˆ†éŸ³é¢‘...")
            cut_audio(media_file, to_del, workspace)
            print("éŸ³é¢‘åˆ‡åˆ†å®Œæˆï¼Œå¼€å§‹åˆå¹¶")
        else:
            print("å¼€å§‹åˆ‡åˆ†è§†é¢‘...")
            cut_video(media_file, to_del, workspace)
            print("è§†é¢‘åˆ‡åˆ†å®Œæˆï¼Œå¼€å§‹åˆå¹¶å’Œå‹ç¼©")
    else:
        print("æ²¡æœ‰éœ€è¦åˆ é™¤çš„ç‰‡æ®µï¼Œç›´æ¥è¿›è¡Œåˆå¹¶")
        # åˆ›å»ºä¸€ä¸ªåŒ…å«å®Œæ•´åª’ä½“æ–‡ä»¶çš„åˆ—è¡¨æ–‡ä»¶
        list_file = workspace / "list.text"
        with open(list_file, "w", encoding="utf-8") as f:
            f.write(f"file '{media_file}'\n")

    return parent


def adjust_subtitles_offset(srt: Path, opening_video: Path, full_srt: Path):
    # dur_ms = int(round(probe_duration(opening_video) * 1000))
    cmd = [
        "ffprobe",
        "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        opening_video,
    ]
    dur_ms = int(round(float(subprocess.check_output(cmd).strip()) * 1000))

    subs = pysubs2.load(str(srt))
    for event in subs.events:
        event.start += dur_ms
        event.end += dur_ms

    subs.save(str(full_srt))


def optimize_subtitles_for_display(srt_file: Path, output_srt: Path = None):
    """
    ä¼˜åŒ–å­—å¹•æ˜¾ç¤ºï¼šè‡ªåŠ¨æ¢è¡Œå’Œè°ƒæ•´è¿‡é•¿çš„å­—å¹•

    Args:
        srt_file: è¾“å…¥å­—å¹•æ–‡ä»¶
        output_srt: è¾“å‡ºå­—å¹•æ–‡ä»¶ï¼Œå¦‚æœä¸ºNoneåˆ™è¦†ç›–åŸæ–‡ä»¶
    """
    if output_srt is None:
        output_srt = srt_file

    subs = pysubs2.load(str(srt_file))

    for event in subs.events:
        text = event.text.strip()
        if not text:
            continue

        # è®¡ç®—å­—ç¬¦é•¿åº¦ï¼ˆä¸­æ–‡å­—ç¬¦æŒ‰2ä¸ªå­—ç¬¦è®¡ç®—ï¼‰
        char_count = sum(2 if ord(c) > 127 else 1 for c in text)

        # æ ¹æ®å­—ç¬¦é•¿åº¦å†³å®šæ˜¯å¦éœ€è¦æ¢è¡Œ
        if char_count > 40:  # è¶…è¿‡40ä¸ªå­—ç¬¦å®½åº¦éœ€è¦æ¢è¡Œ
            # æ™ºèƒ½æ¢è¡Œï¼šä¼˜å…ˆåœ¨æ ‡ç‚¹ç¬¦å·å¤„æ¢è¡Œ
            punctuation = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€'
            words = []
            current_word = ""

            for char in text:
                current_word += char
                if char in punctuation:
                    words.append(current_word)
                    current_word = ""

            if current_word:
                words.append(current_word)

            # é‡æ–°ç»„ç»‡æˆä¸¤è¡Œ
            if len(words) > 1:
                mid_point = len(words) // 2
                line1 = ''.join(words[:mid_point]).strip()
                line2 = ''.join(words[mid_point:]).strip()

                # ç¡®ä¿ä¸¤è¡Œé•¿åº¦ç›¸å¯¹å‡è¡¡
                line1_len = sum(2 if ord(c) > 127 else 1 for c in line1)
                line2_len = sum(2 if ord(c) > 127 else 1 for c in line2)

                if abs(line1_len - line2_len) > 10 and len(words) > 2:
                    # é‡æ–°è°ƒæ•´åˆ†å‰²ç‚¹
                    best_split = mid_point
                    best_diff = abs(line1_len - line2_len)

                    for i in range(1, len(words)):
                        test_line1 = ''.join(words[:i]).strip()
                        test_line2 = ''.join(words[i:]).strip()
                        test_len1 = sum(2 if ord(c) > 127 else 1 for c in test_line1)
                        test_len2 = sum(2 if ord(c) > 127 else 1 for c in test_line2)
                        test_diff = abs(test_len1 - test_len2)

                        if test_diff < best_diff:
                            best_diff = test_diff
                            best_split = i

                    line1 = ''.join(words[:best_split]).strip()
                    line2 = ''.join(words[best_split:]).strip()

                if line1 and line2:
                    event.text = f"{line1}\\N{line2}"
            else:
                # å¦‚æœæ²¡æœ‰æ ‡ç‚¹ç¬¦å·ï¼ŒæŒ‰å­—ç¬¦æ•°å¼ºåˆ¶æ¢è¡Œ
                mid_char = len(text) // 2
                # å¯»æ‰¾æœ€è¿‘çš„ç©ºæ ¼æˆ–åˆé€‚çš„æ–­ç‚¹
                split_point = mid_char
                for i in range(max(0, mid_char - 5), min(len(text), mid_char + 5)):
                    if text[i] in ' \t':
                        split_point = i
                        break

                line1 = text[:split_point].strip()
                line2 = text[split_point:].strip()
                if line1 and line2:
                    event.text = f"{line1}\\N{line2}"

    subs.save(str(output_srt))
    print(f"å­—å¹•æ˜¾ç¤ºä¼˜åŒ–å®Œæˆ: {output_srt}")


def get_adaptive_subtitle_style(srt_file: Path):
    """
    æ ¹æ®å­—å¹•å†…å®¹ç”Ÿæˆè‡ªé€‚åº”çš„å­—å¹•æ ·å¼

    Args:
        srt_file: å­—å¹•æ–‡ä»¶è·¯å¾„

    Returns:
        str: FFmpegå­—å¹•æ ·å¼å­—ç¬¦ä¸²
    """
    subs = pysubs2.load(str(srt_file))

    # åˆ†æå­—å¹•ç‰¹å¾
    max_char_count = 0
    avg_char_count = 0
    total_events = 0

    for event in subs.events:
        if event.text.strip():
            # è®¡ç®—å­—ç¬¦é•¿åº¦ï¼ˆä¸­æ–‡å­—ç¬¦æŒ‰2ä¸ªå­—ç¬¦è®¡ç®—ï¼‰
            char_count = sum(2 if ord(c) > 127 else 1 for c in event.text)
            max_char_count = max(max_char_count, char_count)
            avg_char_count += char_count
            total_events += 1

    if total_events > 0:
        avg_char_count /= total_events

    # æ ¹æ®å­—å¹•é•¿åº¦åŠ¨æ€è°ƒæ•´å­—ä½“å¤§å°
    if max_char_count > 60:
        font_size = 18  # å¾ˆé•¿çš„å­—å¹•ç”¨å°å­—ä½“
    elif max_char_count > 40:
        font_size = 20  # è¾ƒé•¿çš„å­—å¹•ç”¨ä¸­ç­‰å­—ä½“
    elif avg_char_count > 30:
        font_size = 22  # å¹³å‡è¾ƒé•¿ç”¨ç¨å°å­—ä½“
    else:
        font_size = 24  # çŸ­å­—å¹•ç”¨æ­£å¸¸å­—ä½“

    # ç”Ÿæˆæ ·å¼å­—ç¬¦ä¸² - é»‘è‰²å­—ä½“ç™½è‰²è¾¹æ¡†æ–¹æ¡ˆ
    style = (
        f"FontName=WenQuanYi Micro Hei Light,"
        f"FontSize={font_size},"
        f"PrimaryColour=&H00000000,"  # é»‘è‰²å­—ä½“
        f"OutlineColour=&H00FFFFFF," # ç™½è‰²æè¾¹
        f"Outline=10,"                # 10pxç™½è‰²æè¾¹
        f"Shadow=0,"                 # ä¸ä½¿ç”¨é˜´å½±
        f"MarginV=40,"               # åº•éƒ¨è¾¹è·
        f"MarginL=60,"               # å·¦è¾¹è·
        f"MarginR=60,"               # å³è¾¹è·
        f"Alignment=2,"              # åº•éƒ¨å±…ä¸­å¯¹é½
        f"WrapStyle=2"               # æ™ºèƒ½æ¢è¡Œ
    )

    print(f"è‡ªé€‚åº”å­—å¹•æ ·å¼: å­—ä½“å¤§å°={font_size}, æœ€é•¿å­—å¹•={max_char_count}å­—ç¬¦, å¹³å‡é•¿åº¦={avg_char_count:.1f}å­—ç¬¦")
    return style

def merge(output_path: Path = None,
          opening_video_path: Path = None, ending_video_path: Path = None):
    """åˆå¹¶å‰ªè¾‘ç‰‡æ®µã€ç‰‡å¤´ã€ç‰‡å°¾ä»¥åŠå‹ç¼©

    Args:
        working_dir: å·¥ä½œç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä»/tmp/transcript.logæ–‡ä»¶è¯»å–
        output_path: è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨åŸè§†é¢‘ä½ç½®
        opening_video_path: ç‰‡å¤´è§†é¢‘è·¯å¾„
        ending_video_path: ç‰‡å°¾è§†é¢‘è·¯å¾„

    è¿™ä¸€æ­¥ä¹‹åï¼Œæ‰€æœ‰çš„å­—å¹•åŠå‰ªè¾‘éƒ½åº”è¯¥æ­£ç¡®å®Œæˆäº†
    """
    # è¯»å–å·¥ä½œæ—¥å¿—
    log_file = Path("/tmp/transcript.log")
    if not log_file.exists():
        raise FileNotFoundError("æ‰¾ä¸åˆ°å·¥ä½œæ—¥å¿—æ–‡ä»¶")


    log = json.load(open(log_file, "r", encoding="utf-8"))
    working_dir = Path(log["working_dir"])
    name = log["name"]

    # å…¼å®¹æ—§ç‰ˆæœ¬æ—¥å¿—æ ¼å¼
    if "raw_file" in log:
        original_file = Path(log["raw_file"])
        file_type = log.get("file_type", "video")  # é»˜è®¤ä¸ºè§†é¢‘
    else:
        # å…¼å®¹æ—§ç‰ˆæœ¬
        original_file = Path(log["raw_video"])
        file_type = "video"

    cut_srt = working_dir / "cut.srt"
    if not cut_srt.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å‰ªè¾‘åçš„å­—å¹•æ–‡ä»¶: {cut_srt}")

    list_file = working_dir / "cut/list.text"

    if not list_file.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°{'éŸ³é¢‘' if file_type == 'audio' else 'è§†é¢‘'}ç‰‡æ®µåˆ—è¡¨æ–‡ä»¶: {list_file}")

    if file_type == "audio":
        # éŸ³é¢‘å¤„ç†
        main_audio = working_dir / f"main{original_file.suffix}"
        print("åˆå¹¶éŸ³é¢‘ç‰‡æ®µ...")
        command = f"ffmpeg -hide_banner -f concat -safe 0 -i {list_file} -fflags +genpts -c copy -avoid_negative_ts make_zero -v error -y '{main_audio}'"
        execute(command, msg="åˆå¹¶ä¸»éŸ³é¢‘")
        merged_media = main_audio
    else:
        # è§†é¢‘å¤„ç†
        main_video = working_dir / "main.mp4"
        print("åˆå¹¶è§†é¢‘ç‰‡æ®µ...")
        command = f"ffmpeg -hide_banner -f concat -safe 0 -i {list_file} -fflags +genpts -c copy -map 0:v -map 0:a -movflags +faststart -video_track_timescale 600 -f mp4 -avoid_negative_ts make_zero -v error -y '{main_video}'"
        execute(command, msg="åˆå¹¶ä¸»è§†é¢‘")
        merged_media = main_video

    if file_type == "video":
        # è§†é¢‘å¤„ç†ï¼šæ”¯æŒç‰‡å¤´ç‰‡å°¾
        video_files = []

        # æ·»åŠ ç‰‡å¤´
        if opening_video_path and Path(opening_video_path).exists():
            video_files.append(Path(opening_video_path))
            print(f"æ·»åŠ ç‰‡å¤´è§†é¢‘: {opening_video_path}")

        # æ·»åŠ ä¸»è§†é¢‘
        video_files.append(merged_media)

        # æ·»åŠ ç‰‡å°¾
        if ending_video_path and Path(ending_video_path).exists():
            video_files.append(Path(ending_video_path))
            print(f"æ·»åŠ ç‰‡å°¾è§†é¢‘: {ending_video_path}")

        # å¦‚æœæœ‰ç‰‡å¤´æˆ–ç‰‡å°¾ï¼Œéœ€è¦é‡æ–°åˆå¹¶
        if len(video_files) > 1:
            merge_list = working_dir / "full.text"
            merged_video = working_dir / f"full.mp4"

            with open(merge_list, "w", encoding="utf-8") as f:
                for video_file in video_files:
                    f.write(f"file '{video_file}'\n")

            # é¦–å…ˆå°è¯•ä½¿ç”¨copyæ¨¡å¼åˆå¹¶ï¼Œé¿å…é‡æ–°ç¼–ç 
            cmd_copy = f"ffmpeg -hide_banner -f concat -safe 0 -i {merge_list} -fflags +genpts -c copy -map 0:v -map 0:a -movflags +faststart -y -f mp4 -video_track_timescale 600 -v error {merged_video}"

            try:
                execute(cmd_copy, msg="åˆå¹¶å®Œæ•´è§†é¢‘(copyæ¨¡å¼)")
            except RuntimeError:
                print("âš ï¸ copyæ¨¡å¼å¤±è´¥ï¼Œå°è¯•é‡æ–°ç¼–ç ...")
                # å¦‚æœcopyæ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨é‡æ–°ç¼–ç 
                cmd_encode = f"ffmpeg -hide_banner -f concat -safe 0 -i {merge_list} -fflags +genpts -c:v libx264 -preset medium -crf 23 -c:a aac -b:a 128k -map 0:v -map 0:a -movflags +faststart -y -f mp4 -video_track_timescale 600 -v error {merged_video}"
                execute(cmd_encode, msg="åˆå¹¶å®Œæ•´è§†é¢‘(é‡æ–°ç¼–ç )")

            # å¦‚æœæœ‰ç‰‡å¤´ï¼Œéœ€è¦è°ƒæ•´å­—å¹•æ—¶é—´åç§»
            if opening_video_path:
                adjusted_srt = working_dir / "adjusted.srt"
                adjust_subtitles_offset(cut_srt, opening_video_path, adjusted_srt)
                cut_srt = adjusted_srt

            merged_media = merged_video
        # å¦‚æœæ²¡æœ‰ç‰‡å¤´ç‰‡å°¾ï¼Œmerged_media å·²ç»æ˜¯æ­£ç¡®çš„æ–‡ä»¶

    # è‡ªåŠ¨å¯¹é½å­—å¹• - è¿™æ˜¯å…³é”®æ­¥éª¤
    print("å¼€å§‹å­—å¹•å¯¹é½...")
    aligned_srt = working_dir / "aligned.srt"
    align_subtitles_with_audio(merged_media, cut_srt, aligned_srt)

    # ç¡®å®šæœ€ç»ˆè¾“å‡ºè·¯å¾„
    if output_path is None:
        # ä½¿ç”¨åŸæ–‡ä»¶çš„ç›®å½•ï¼Œæ–‡ä»¶ååŠ åç¼€
        output_dir = original_file.parent
        base_name = original_file.stem
    else:
        output_path = Path(output_path)
        if output_path.is_dir():
            output_dir = output_path
            base_name = name
        else:
            output_dir = output_path.parent
            base_name = output_path.stem

    # ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶è·¯å¾„
    if file_type == "audio":
        # éŸ³é¢‘æ–‡ä»¶è¾“å‡º
        final_audio = output_dir / f"{base_name}-final{original_file.suffix}"
        final_srt = output_dir / f"{base_name}-final.srt"

        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        print(f"å‰ªè¾‘éŸ³é¢‘: {final_audio}")
        print(f"å­—å¹•æ–‡ä»¶: {final_srt}")

        # å¤åˆ¶å‰ªè¾‘åçš„éŸ³é¢‘æ–‡ä»¶
        print("å¤åˆ¶å‰ªè¾‘åçš„éŸ³é¢‘...")
        shutil.copy2(merged_media, final_audio)

        # å¤åˆ¶å­—å¹•æ–‡ä»¶
        shutil.copy2(aligned_srt, final_srt)

        print("\n=== éŸ³é¢‘å¤„ç†å®Œæˆ ===")
        print(f"âœ… å‰ªè¾‘éŸ³é¢‘: {final_audio}")
        print(f"âœ… å­—å¹•æ–‡ä»¶: {final_srt}")

        return final_audio, None, final_srt
    else:
        # è§†é¢‘æ–‡ä»¶è¾“å‡º
        final_with_sub = output_dir / f"{base_name}-final-sub.mp4"
        final_no_sub = output_dir / f"{base_name}-final.mp4"
        final_srt = output_dir / f"{base_name}-final.srt"

        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        print(f"å¸¦å­—å¹•ç‰ˆæœ¬: {final_with_sub}")
        print(f"æ— å­—å¹•ç‰ˆæœ¬: {final_no_sub}")

        # ä¼˜åŒ–å­—å¹•æ˜¾ç¤º
        print("ä¼˜åŒ–å­—å¹•æ˜¾ç¤º...")
        optimized_srt = working_dir / "optimized.srt"
        optimize_subtitles_for_display(aligned_srt, optimized_srt)

        # å‹ç¼©åŠçƒ§å­—å¹•
        print("ç”Ÿæˆå¸¦å­—å¹•ç‰ˆæœ¬...")

        # è·å–è‡ªé€‚åº”å­—å¹•æ ·å¼
        subtitle_style = get_adaptive_subtitle_style(optimized_srt)

        cmd = f"ffmpeg -hide_banner -i {merged_media} -vf \"subtitles={optimized_srt}:force_style='{subtitle_style}'\" -c:v libx264 -preset slow -crf 23 -c:a copy -v error -y '{final_with_sub}'"
        execute(cmd, msg="ç”Ÿæˆå¸¦å­—å¹•ç‰ˆæœ¬")

        # å‹ç¼©æœªåŠ å­—å¹•çš„è§†é¢‘
        print("ç”Ÿæˆæ— å­—å¹•ç‰ˆæœ¬...")
        cmd = f"ffmpeg -hide_banner -i {merged_media} -c:v libx264 -preset slow -crf 23 -c:a copy -v error -y '{final_no_sub}'"
        execute(cmd, msg="ç”Ÿæˆæ— å­—å¹•ç‰ˆæœ¬")

        # å¤åˆ¶å­—å¹•æ–‡ä»¶
        shutil.copy2(aligned_srt, final_srt)

        print("\n=== è§†é¢‘å¤„ç†å®Œæˆ ===")
        print(f"âœ… å¸¦å­—å¹•è§†é¢‘: {final_with_sub}")
        print(f"âœ… æ— å­—å¹•è§†é¢‘: {final_no_sub}")
        print(f"âœ… å­—å¹•æ–‡ä»¶: {final_srt}")

        return final_with_sub, final_no_sub, final_srt


def t2s(srt: str):
    """å°†ç¹ä¸­è½¬æ¢ä¸ºç®€ä¸­"""
    srt_file = Path(srt)
    if not srt_file.exists():
        raise FileNotFoundError(f"å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {srt_file}")

    subs = pysubs2.load(str(srt_file))

    converter = opencc.OpenCC("t2s.json")
    for event in subs.events:
        text = event.text

        replaced = converter.convert(text)
        if event.text != replaced:
            print(f"{event.text} -> {replaced}")
        event.text = replaced

    subs.save(str(srt_file))

def process_video(input_video: str, output_path: str = None,
                 opening_video: str = None, ending_video: str = None,
                 auto_process: bool = False):
    """
    å®Œæ•´çš„è§†é¢‘å¤„ç†æµç¨‹

    Args:
        input_video: è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆç›®å½•æˆ–æ–‡ä»¶è·¯å¾„ï¼‰
        opening_video: ç‰‡å¤´è§†é¢‘è·¯å¾„
        ending_video: ç‰‡å°¾è§†é¢‘è·¯å¾„
        auto_process: æ˜¯å¦è‡ªåŠ¨å¤„ç†ï¼ˆè·³è¿‡æ‰‹åŠ¨ç¼–è¾‘æ­¥éª¤ï¼‰
    """
    try:
        print("=== å¼€å§‹è§†é¢‘å­—å¹•å¤„ç†æµç¨‹ ===")

        # æ­¥éª¤1: ç”Ÿæˆå­—å¹•
        print("\næ­¥éª¤1: ç”Ÿæˆå­—å¹•")
        srt_file = transcript(Path(input_video))

        if not auto_process:
            print(f"\nè¯·ç¼–è¾‘å­—å¹•æ–‡ä»¶: {srt_file}")
            print("- å¯ä»¥åˆ é™¤ä¸éœ€è¦çš„å­—å¹•è¡Œ")
            print("- åœ¨éœ€è¦åˆ é™¤çš„å­—å¹•å‰æ·»åŠ  [DEL] æ ‡è®°")
            print("- ç¼–è¾‘å®ŒæˆåæŒ‰å›è½¦ç»§ç»­...")
            input("æŒ‰å›è½¦é”®ç»§ç»­...")

        # æ­¥éª¤2: å‰ªè¾‘è§†é¢‘
        print("\næ­¥éª¤2: å‰ªè¾‘è§†é¢‘")
        cut()

        # æ­¥éª¤3: åˆå¹¶å’Œè¾“å‡º
        print("\næ­¥éª¤3: åˆå¹¶å’Œè¾“å‡º")
        final_with_sub, final_no_sub, final_srt = merge(
            output_path=Path(output_path) if output_path else None,
            opening_video_path=Path(opening_video) if opening_video else None,
            ending_video_path=Path(ending_video) if ending_video else None
        )

        print("\n=== å¤„ç†å®Œæˆ ===")
        return final_with_sub, final_no_sub, final_srt

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        raise


def resume(output_path: str = None, opening_video: str = None, ending_video: str = None):
    """
    ç¼–è¾‘å­—å¹•åç»§ç»­å¤„ç†ï¼ˆè‡ªåŠ¨è°ƒç”¨alignï¼‰

    Args:
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆç›®å½•æˆ–æ–‡ä»¶è·¯å¾„ï¼‰
        opening_video: ç‰‡å¤´è§†é¢‘è·¯å¾„
        ending_video: ç‰‡å°¾è§†é¢‘è·¯å¾„
    """
    try:
        print("=== ç»§ç»­å¤„ç†å·²ç¼–è¾‘çš„å­—å¹• ===")
        print("å°†è‡ªåŠ¨è°ƒç”¨å­—å¹•å¯¹é½åŠŸèƒ½")

        # æ­¥éª¤1: å‰ªè¾‘è§†é¢‘
        print("\næ­¥éª¤1: å‰ªè¾‘è§†é¢‘")
        cut()

        # æ­¥éª¤2: åˆå¹¶å’Œè¾“å‡ºï¼ˆmergeå‡½æ•°å†…éƒ¨ä¼šè‡ªåŠ¨è°ƒç”¨alignï¼‰
        print("\næ­¥éª¤2: åˆå¹¶å’Œè¾“å‡º")
        final_with_sub, final_no_sub, final_srt = merge(
            output_path=Path(output_path) if output_path else None,
            opening_video_path=Path(opening_video) if opening_video else None,
            ending_video_path=Path(ending_video) if ending_video else None
        )

        print("\n=== å¤„ç†å®Œæˆ ===")
        print(f"âœ… å¸¦å­—å¹•è§†é¢‘: {final_with_sub}")
        print(f"âœ… æ— å­—å¹•è§†é¢‘: {final_no_sub}")
        print(f"âœ… å­—å¹•æ–‡ä»¶: {final_srt}")

        return final_with_sub, final_no_sub, final_srt

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        raise


def auto(input_video: str, output_path: str = None,
         opening_video: str = None, ending_video: str = None):
    """
    è‡ªåŠ¨å¤„ç†æµç¨‹ï¼ˆæ— éœ€æ‰‹åŠ¨ç¼–è¾‘å­—å¹•ï¼‰

    Args:
        input_video: è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆç›®å½•æˆ–æ–‡ä»¶è·¯å¾„ï¼‰
        opening_video: ç‰‡å¤´è§†é¢‘è·¯å¾„
        ending_video: ç‰‡å°¾è§†é¢‘è·¯å¾„
    """
    return process_video(input_video, output_path, opening_video, ending_video, auto_process=True)


def test():
    """æµ‹è¯•æ¨¡å‹åŠ è½½åŠŸèƒ½"""
    import os
    print("HF_ENDPOINT:", os.environ.get("HF_ENDPOINT"))
    print("HF_HOME:", hf_home)
    print("Model directory:", model_dir)

    # è®¾ç½®ç¦»çº¿æ¨¡å¼
    os.environ["TRANSFORMERS_OFFLINE"] = "1"
    os.environ["HF_HUB_OFFLINE"] = "1"

    device = "cpu"
    try:
        print("Loading align model...")

        # å°è¯•ä½¿ç”¨æœ¬åœ°è·¯å¾„
        local_model_path = Path(model_dir) / "models--jonatasgrosman--wav2vec2-large-xlsr-53-chinese-zh-cn"
        if local_model_path.exists():
            # æŸ¥æ‰¾æœ€æ–°çš„å¿«ç…§ç›®å½•
            snapshots_dir = local_model_path / "snapshots"
            if snapshots_dir.exists():
                snapshot_dirs = [d for d in snapshots_dir.iterdir() if d.is_dir()]
                if snapshot_dirs:
                    latest_snapshot = max(snapshot_dirs, key=lambda x: x.stat().st_mtime)
                    print(f"Using local model from: {latest_snapshot}")
                    model_name = str(latest_snapshot)
                else:
                    model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn"
            else:
                model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn"
        else:
            model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn"

        model_a, metadata = whisperx.load_align_model(
            language_code="zh",
            device=device,
            model_name=model_name,
            model_dir=hf_home
        )
        print("âœ… Model loaded successfully!")
        print(f"Model type: {type(model_a)}")
        print(f"Metadata: {metadata}")
    except Exception as e:
        print(f"âŒ Model loading failed: {e}")
        print("Please check if the model files are properly downloaded.")

# CLIå…¥å£ç‚¹å·²ç§»è‡³ transcript/cli.py
# å¦‚éœ€ä½¿ç”¨åŸç‰ˆå‘½ä»¤ï¼Œè¯·ç›´æ¥å¯¼å…¥ç›¸åº”å‡½æ•°
