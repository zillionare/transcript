#! /Users/aaronyang/miniforge3/envs/coursea/bin/python

"""
todo: 

1. åœ¨è¿›è¡Œç¬¬ä¸€è½®transcriptä¹‹å‰ï¼Œå°±å¯¹è¾“å…¥è§†é¢‘è¿›è¡Œåˆ‡åˆ†(a)ï¼Œä½¿å¾—æ¯ä¸€æ®µä¸é•¿äº15åˆ†é’Ÿã€‚å°†åˆ†å‰²çš„æ–‡ä»¶ä¿å­˜åˆ° /tmp/$working_dir/cuts
2. å¯¹cutsé‡Œçš„æ¯ä¸€ä¸ªæ–‡ä»¶ï¼Œè¿›è¡Œtranscriptï¼Œç„¶åç­‰å¾…äººå·¥æ ¡å¯¹
3. äººå·¥æ ¡æ­£çš„srtå¯èƒ½åŒ…å«ç¼–è¾‘æŒ‡ä»¤ï¼Œæ¯”å¦‚[del],å’Œè‡ªåŠ¨åˆ é™¤å•è¡Œçš„ã€å‘ƒã€æ©ã€å¯¹ï¼Œå¥½ã€ç­‰ï¼Œè¿™æ ·ä¼šç”Ÿæˆä¸€äº›å°çš„ç¢ç‰‡ï¼Œé€šè¿‡ffmpegè¿›è¡Œå‰ªè¾‘ã€åˆå¹¶ã€‚è¿™ä¸€æ­¥çš„ç»“æœä¿å­˜åˆ°/tmp/$working_dir/reviewed
4. å°†reviewedçš„æ–‡ä»¶ã€åŠ å…¥ç‰‡å¤´ç‰‡å°¾ï¼Œåˆå¹¶ã€å‹ç¼©å’Œä¿å­˜
5. å°†reviewedä¸­çš„æ–‡ä»¶ï¼Œçƒ§å…¥å­—å¹•ï¼ŒåŠ ç‰‡å¤´ç‰‡å°¾ï¼Œåˆå¹¶ã€å‹ç¼©å’Œä¿å­˜


å…¶å®ƒï¼š
1. å¯¹1ä¸­äº§ç”Ÿçš„cutsï¼Œæ¯å¤„ç†ä¸€ä¸ªï¼Œéƒ½è¦æ˜¾ç¤º æ­£åœ¨å¤„ç† i/total åŠ å·²å®Œæˆ

å‚è€ƒï¼š
a: è¿™ä¸ªå‘½ä»¤èƒ½æŒ‰å…³é”®å¸§åˆ†å‰²ï¼š ffmpeg -i raw.mp4 -c copy -map 0 -segment_time 00:15:00 -f segment -break_non_keyframes 0 output%03d.mp4

"""

import datetime
import json
import os
import shlex
import shutil
import subprocess
import sys
import warnings
from multiprocessing.util import is_exiting
from pathlib import Path
from subprocess import PIPE, STDOUT, Popen
from typing import Any, List, Tuple

# fireå·²ç§»é™¤ï¼Œä½¿ç”¨CLIæ¥å£
import jieba
import opencc
import pysubs2

# from pywhispercpp.model import Model


warnings.filterwarnings("ignore")

import whisperx

prompt = ",".join(["å¤§å®¶å¥½ï¼Œæˆ‘ä»¬å¼€å§‹ä¸Šè¯¾äº†ã€‚è¯·è¾“å‡ºç®€ä½“ä¸­æ–‡ã€‚"])

opening_video = Path("/Volumes/share/data/autobackup/ke/factor-ml/opening.mp4")
ending_video = Path("/Volumes/share/data/autobackup/ke/factor-ml/end.mp4")

cpp_path = Path("/Volumes/share/data/whisper.cpp")
cpp_model = Path("/Volumes/share/data/whisper.cpp/models/ggml-large-v2.bin")
# è®¾ç½®æœ¬åœ°æ¨¡å‹ç›®å½•ï¼Œå¦‚æœç¯å¢ƒå˜é‡æœªè®¾ç½®åˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„
model_dir = os.environ.get("hf_model_dir", "/Volumes/share/data/models/huggingface/hub")

# è®¾ç½®whisperxæ¨¡å‹åç§°
whisperx_model = "large-v2"  # æ”¯æŒä¸­æ–‡çš„whisperæ¨¡å‹
w2v_model = "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn"  # ä¸­æ–‡å¯¹é½æ¨¡å‹

def align_subtitles_with_audio(video: Path, original_srt: Path, aligned_srt: Path):
    """
    ä½¿ç”¨ whisperx å¯¹é½å­—å¹•æ–‡ä»¶ä¸éŸ³é¢‘ã€‚
    è¿™æ˜¯ç¡®ä¿å‰ªè¾‘åè§†é¢‘ä¸å­—å¹•åŒæ­¥çš„å…³é”®æ­¥éª¤ã€‚

    Args:
        video: åˆå¹¶åçš„è§†é¢‘æ–‡ä»¶è·¯å¾„
        original_srt: åŸå§‹å­—å¹•æ–‡ä»¶è·¯å¾„
        aligned_srt: å¯¹é½åçš„å­—å¹•æ–‡ä»¶è·¯å¾„
    """
    print(f"å¼€å§‹å­—å¹•å¯¹é½: {original_srt} -> {aligned_srt}")

    try:
        # éªŒè¯è¾“å…¥æ–‡ä»¶
        if not Path(video).exists():
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video}")
        if not Path(original_srt).exists():
            raise FileNotFoundError(f"å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {original_srt}")

        # æå–éŸ³é¢‘
        audio_path = Path(video).with_suffix(".wav")
        if not audio_path.exists():
            print("æå–éŸ³é¢‘æ–‡ä»¶...")
            extract_audio(video, audio_path)

        # è®¾ç½®è®¾å¤‡ - Mac ARMä¼˜åŒ–
        import platform
        if platform.system() == "Darwin" and platform.machine() == "arm64":
            # Mac ARMæ¶æ„ï¼Œä½¿ç”¨CPU
            device = "cpu"
            print("æ£€æµ‹åˆ°Mac ARMæ¶æ„ï¼Œä½¿ç”¨CPUè¿›è¡Œå¯¹é½")
        else:
            device = "cpu"  # ä¿æŒå…¼å®¹æ€§

        print("åŠ è½½éŸ³é¢‘...")
        audio = whisperx.load_audio(str(audio_path))

        # åŠ è½½å­—å¹•æ–‡ä»¶
        print("åŠ è½½å­—å¹•æ–‡ä»¶...")
        subs = pysubs2.load(str(original_srt))

        if not subs.events:
            print("è­¦å‘Š: å­—å¹•æ–‡ä»¶ä¸ºç©º")
            shutil.copy2(original_srt, aligned_srt)
            return

        segments = []
        for i, event in enumerate(subs.events):
            if event.text.strip():  # è·³è¿‡ç©ºå­—å¹•
                segments.append({
                    "start": event.start / 1000,
                    "end": event.end / 1000,
                    "text": event.text.strip(),
                    "id": i
                })

        if not segments:
            print("è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„å­—å¹•æ®µè½")
            shutil.copy2(original_srt, aligned_srt)
            return

        # è®¾ç½®ç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡
        import os
        os.environ["TRANSFORMERS_OFFLINE"] = "1"
        os.environ["HF_HUB_OFFLINE"] = "1"

        # åŠ è½½å¯¹é½æ¨¡å‹
        print("åŠ è½½å¯¹é½æ¨¡å‹...")
        model_name = None

        # å°è¯•ä½¿ç”¨æœ¬åœ°è·¯å¾„
        local_model_path = Path(model_dir) / "models--jonatasgrosman--wav2vec2-large-xlsr-53-chinese-zh-cn"
        if local_model_path.exists():
            snapshots_dir = local_model_path / "snapshots"
            if snapshots_dir.exists():
                snapshot_dirs = [d for d in snapshots_dir.iterdir() if d.is_dir()]
                if snapshot_dirs:
                    latest_snapshot = max(snapshot_dirs, key=lambda x: x.stat().st_mtime)
                    print(f"ä½¿ç”¨æœ¬åœ°æ¨¡å‹: {latest_snapshot}")
                    model_name = str(latest_snapshot)

        if model_name is None:
            model_name = w2v_model
            print(f"ä½¿ç”¨é»˜è®¤æ¨¡å‹åç§°: {model_name}")

        try:
            model_a, metadata = whisperx.load_align_model(
                language_code="zh",
                device=device,
                model_name=model_name,
                model_dir=model_dir
            )
            print("å¯¹é½æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå¼€å§‹å¯¹é½...")
        except Exception as model_error:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {model_error}")
            raise

        # æ‰§è¡Œå¯¹é½
        print(f"å¯¹é½ {len(segments)} ä¸ªå­—å¹•æ®µè½...")
        aligned_result = whisperx.align(segments, model_a, metadata, audio, device)

        if "segments" not in aligned_result or not aligned_result["segments"]:
            print("è­¦å‘Š: å¯¹é½ç»“æœä¸ºç©º")
            shutil.copy2(original_srt, aligned_srt)
            return

        # åˆ›å»ºå¯¹é½åçš„å­—å¹•
        aligned_subs = pysubs2.SSAFile()

        for i, segment in enumerate(aligned_result["segments"]):
            if "start" in segment and "end" in segment and "text" in segment:
                event = pysubs2.SSAEvent()
                event.start = int(segment["start"] * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                event.end = int(segment["end"] * 1000)
                event.text = segment["text"]

                # ç¡®ä¿æ—¶é—´æˆ³åˆç†
                if event.end <= event.start:
                    event.end = event.start + 1000  # è‡³å°‘1ç§’

                # é¿å…é‡å 
                if i > 0 and event.start < aligned_subs.events[-1].end:
                    event.start = aligned_subs.events[-1].end + 100
                    if event.end <= event.start:
                        event.end = event.start + 1000

                aligned_subs.events.append(event)

        # ä¿å­˜å¯¹é½åçš„å­—å¹•æ–‡ä»¶
        aligned_subs.save(str(aligned_srt))
        print(f"âœ… å­—å¹•å¯¹é½å®Œæˆ: {len(aligned_subs.events)} ä¸ªæ®µè½")
        print(f"å¯¹é½ç»“æœä¿å­˜åˆ°: {aligned_srt}")

    except Exception as e:
        print(f"âŒ å­—å¹•å¯¹é½å¤±è´¥: {e}")
        print("ä½¿ç”¨åŸå§‹å­—å¹•æ–‡ä»¶ä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ...")

        # ç›´æ¥å¤åˆ¶åŸå§‹å­—å¹•æ–‡ä»¶
        shutil.copy2(original_srt, aligned_srt)
        print(f"å·²å¤åˆ¶åŸå§‹å­—å¹•åˆ°: {aligned_srt}")
        print("ğŸ’¡ æç¤º: è¦ä½¿ç”¨å­—å¹•å¯¹é½åŠŸèƒ½ï¼Œè¯·ç¡®ä¿:")
        print("   1. è¿è¡Œ 'python download_models.py' ä¸‹è½½å¯¹é½æ¨¡å‹")
        print("   2. æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å®Œæ•´")
        print("   3. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸ï¼ˆé¦–æ¬¡ä¸‹è½½æ—¶ï¼‰")


def execute(cmd, dry_run=False, supress_log=False, msg: str = ""):
    start = datetime.datetime.now()
    if not supress_log:
        print(f">>> {msg} {cmd}")

    if dry_run:
        return

    if isinstance(cmd, str):
        args = shlex.split(cmd)
    else:
        args = cmd
    with Popen(args, stdout=PIPE, stderr=STDOUT, text=True) as proc:
        for line in proc.stdout:  # type: ignore
            print(line)

    if proc.returncode != 0:
        raise RuntimeError(f"FFmpeg Error {proc.returncode}: {proc.stderr}")

    if not supress_log:
        cost(start, prefix=f"<<< {msg} Done ")


def _ms_to_hms(ms: int):
    ms_ = ms / 1000
    h = int(ms_ // 3600)
    m = int((ms_ - h * 3600) // 60)
    s = int((ms_ - h * 3600 - m * 60))

    return f"{h:02d}:{m:02d}:{s:02d}.{ms % 1000:03d}"


def transcript_cpp(input_audio: Path, output_srt: Path, prompt: str, dry_run=False):
    """
    Args:
        input_audio (str): _description_
        output_srt (str): _description_
        prompt (str): _description_
    """
    whisper = os.path.join(cpp_path, "whisper-cli")

    cmd = f"{whisper} {input_audio} -l zh -sow -ml 30 -t 8 -m {cpp_model} -osrt -of {output_srt.with_suffix('')} --prompt '{prompt}'"
    execute(cmd)


def init_jieba():
    custom_map = {}
    dict_path = Path(__file__).parent.parent / "dict.md"
    with open(dict_path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if line.startswith("#") or len(line) == 0:
                continue
            wrong, right = line.split(",")
            custom_map[wrong] = right

    for word in custom_map.keys():
        jieba.add_word(word)

    return custom_map


def transcriptx(input_audio: Path, output_srt: Path, prompt: str):
    compute_type = "default"
    device = "cpu"

    options = {"initial_prompt": prompt}
    model = whisperx.load_model(
        whisperx_model,
        device=device,
        compute_type=compute_type,
        asr_options=options,
        language="zh",
        threads=8,
    )

    audio = whisperx.load_audio(input_audio)
    result = model.transcribe(
        audio, language="zh", print_progress=True, batch_size=8, chunk_size=10
    )

    subs = pysubs2.load_from_whisper(result["segments"])
    subs.save(str(output_srt))
    # 2. Align whisper output
    # model_a, metadata = whisperx.load_align_model(
    #     language_code=result["language"], device=device, model_name = w2v_model
    # )
    # result = whisperx.align(
    #     result["segments"],
    #     model_a,
    #     metadata,
    #     audio,
    #     device,
    #     return_char_alignments=False,
    # )

    # print(result["segments"])  # after alignment


def extract_audio(input_video: Path, output_wav: Path):
    cmd = (
        f"ffmpeg -i {input_video} -vn -acodec pcm_s16le -ar 16000 -ac 2 -y {output_wav} -v error"
    )
    execute(cmd)


def cost(start, cmd: str = "", prefix=""):
    end = datetime.datetime.now()
    elapsed = (end - start).total_seconds()
    mins = elapsed // 60
    seconds = elapsed % 60
    print(
        f"{prefix}{end.hour:02d}:{end.minute:02d}:{end.second:02d} {cmd}ç”¨æ—¶{mins}åˆ†{seconds:.0f}ç§’"
    )


def transcript(input_video: Path, output_dir: Path = None, dry_run=False):
    """
    å°†è§†é¢‘è½¬æ¢ä¸ºå­—å¹•æ–‡ä»¶

    Args:
        input_video: è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™å°†srtæ–‡ä»¶ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•
        dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼
    """
    input_video = Path(input_video)

    # éªŒè¯è¾“å…¥æ–‡ä»¶å­˜åœ¨
    if not input_video.exists():
        raise FileNotFoundError(f"è¾“å…¥è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {input_video}")

    # éªŒè¯æ–‡ä»¶æ ¼å¼
    valid_extensions = {'.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv'}
    if input_video.suffix.lower() not in valid_extensions:
        raise ValueError(f"ä¸æ”¯æŒçš„è§†é¢‘æ ¼å¼: {input_video.suffix}")

    # ä½¿ç”¨è§†é¢‘æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ä½œä¸ºé¡¹ç›®åç§°
    name = input_video.stem

    # è®¾ç½®å·¥ä½œç›®å½•ï¼ˆç”¨äºä¸´æ—¶æ–‡ä»¶ï¼‰
    working_dir = Path("/tmp/transcript") / name

    # è®¾ç½®æœ€ç»ˆsrtæ–‡ä»¶çš„è¾“å‡ºç›®å½•
    if output_dir is None:
        # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆtranscript.pyæ‰€åœ¨ç›®å½•çš„ä¸Šçº§ç›®å½•ï¼‰
        script_dir = Path(__file__).parent
        project_root = script_dir.parent
        final_output_dir = project_root
    else:
        final_output_dir = Path(output_dir)

    working_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜å·¥ä½œæ—¥å¿—
    log_file = "/tmp/transcript.log"
    with open(log_file, "w", encoding="utf-8") as f:
        json.dump({
            "working_dir": str(working_dir),
            "name": name,
            "raw_video": str(input_video),
            "timestamp": datetime.datetime.now().isoformat()
            }, f, indent=2)

    # å¤åˆ¶è§†é¢‘åˆ°å·¥ä½œç›®å½•
    video = working_dir / input_video.name
    if not video.exists():
        print(f"å¤åˆ¶è§†é¢‘æ–‡ä»¶åˆ°å·¥ä½œç›®å½•: {input_video} -> {video}")
        shutil.copy(input_video, video)

    # è®¾ç½®ä¸´æ—¶srtæ–‡ä»¶è·¯å¾„ï¼ˆåœ¨å·¥ä½œç›®å½•ä¸­ï¼‰
    temp_srt = working_dir / f"{name}.srt"
    output_wav = video.with_suffix(".wav")

    # è®¾ç½®æœ€ç»ˆsrtæ–‡ä»¶è·¯å¾„ï¼ˆåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸­ï¼‰
    final_srt = final_output_dir / f"{name}.srt"

    print(f"å¼€å§‹è½¬æ¢è§†é¢‘ä¸ºå­—å¹•: {input_video} -> {final_srt}")

    # æå–éŸ³é¢‘
    start = datetime.datetime.now()
    print(f"{start.hour:02d}:{start.minute:02d}:{start.second:02d}: å¼€å§‹å¤„ç†")

    if not output_wav.exists():
        print("æå–éŸ³é¢‘...")
        extract_audio(video, output_wav)

    # ç”Ÿæˆå­—å¹•åˆ°ä¸´æ—¶ä½ç½®
    print("ç”Ÿæˆå­—å¹•...")
    transcript_cpp(output_wav, temp_srt, prompt, dry_run)

    # åº”ç”¨è‡ªå®šä¹‰è¯å…¸çº é”™
    print("åº”ç”¨è¯å…¸çº é”™...")
    sub(temp_srt)

    # å¤åˆ¶å­—å¹•æ–‡ä»¶åˆ°é¡¹ç›®æ ¹ç›®å½•
    print(f"å¤åˆ¶å­—å¹•æ–‡ä»¶åˆ°é¡¹ç›®æ ¹ç›®å½•: {temp_srt} -> {final_srt}")
    shutil.copy2(temp_srt, final_srt)

    cost(start, prefix="å­—å¹•ç”Ÿæˆå®Œæˆ ")
    print(f"å­—å¹•æ–‡ä»¶å·²ä¿å­˜åˆ°: {final_srt}")
    print(f"å·¥ä½œç›®å½•: {working_dir}")

    return final_srt


def probe_duration(video):
    """returns duration of a video in seconds"""
    cmd = [
        "ffprobe",
        "-v",
        "error",
        "-show_entries",
        "format=duration",
        "-of",
        "default=noprint_wrappers=1:nokey=1",
        video,
    ]
    duration = float(subprocess.check_output(cmd).strip())
    return duration


def cut_video(input_video: Path, to_del: List[Tuple[int, int, int]], out_dir: Path):
    """æ ¹æ®sliceså‰ªå»è§†é¢‘ç‰‡æ®µ

    ffmpeg -hide_banner -ss '60.08300' -i '/private/tmp/fa.mp4' -t '178.00000' -avoid_negative_ts make_zero -map '0:0' '-c:0' copy -map '0:1' '-c:1' copy -map_metadata 0 -movflags '+faststart' -default_mode infer_no_subs -ignore_unknown -f mp4 -y '/private/tmp/fa-00.01.00.083-00.03.58.083-seg2.mp4'
    """
    duration = probe_duration(input_video) * 1000

    edges = [item for sublist in to_del for item in sublist[1:]]
    edges.insert(0, 0)
    edges.append(int(duration))

    if edges[:2] == [0, 0]:
        edges = edges[2:]

    slices = [(edges[i], edges[i + 1]) for i in range(0, len(edges) - 1, 2)]

    tmp_files = []

    for i, (start, end) in enumerate(slices):
        start_time = _ms_to_hms(start)
        end_time = _ms_to_hms(end)
        start_secs = f"{start/1000:.3f}"
        dur = f"'{(end - start)/1000:.3f}'"

        tmp_file = os.path.join(out_dir, f"{i}-{start_time}-{end_time}.mp4")
        tmp_files.append(tmp_file)
        cmd = f"ffmpeg -hide_banner -ss '{start_secs}' -i '{input_video}' -t '{dur}' -c copy -map 0:v -map 0:a -map_metadata 0 -movflags '+faststart' -f mp4 -video_track_timescale 600 -y -v error '{tmp_file}'"

        execute(cmd, supress_log=True)

    list_file = os.path.join(out_dir, "list.text")
    with open(list_file, "w", encoding="utf-8") as f:
        for video in tmp_files:
            f.write(f"file '{video}'\n")

    return list_file


def sub(srt_file: Path):
    """è¾“å…¥å­—å¹•æ–‡ä»¶ï¼Œé€šè¿‡è‡ªå®šä¹‰è¯å…¸ï¼Œå…ˆç²—ç­›ä¸€æ¬¡ã€‚åœ¨transcriptä¹‹åè¢«è‡ªåŠ¨è°ƒç”¨ã€‚

    Args:
        srt_file (str): _description_
    """
    subs = pysubs2.load(str(srt_file))

    custom_map = init_jieba()
    for event in subs.events:
        text = event.text

        replaced = "".join([custom_map.get(x, x) for x in jieba.cut(text)])
        if event.text != replaced:
            print(f"{event.text} -> {replaced}")
        event.text = replaced

    subs.save(str(srt_file))


def cut():
    """å­—å¹•ç¼–è¾‘ä¹‹åï¼Œè¿›è¡Œè§†é¢‘åˆ‡åˆ†ã€åˆå¹¶ã€å‹å­—å¹•ã€å‹ç¼©åŠæ‹·è´

    1. å‰ªæ‰è¯­åŠ©ï¼ˆå•è¡Œçš„å¥½ï¼Œå‘ƒç­‰ï¼‰
    2. å‰ªæ‰å­—å¹•ä¸­ä»¥[del]å¼€å¤´çš„event
    3. æ ¹æ®è‡ªå®šä¹‰è¯å…¸å®Œæˆæ›¿æ¢

    Args:
        working_dir: å·¥ä½œç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä»/tmp/transcript.logæ–‡ä»¶è¯»å–
    """
    # è¯»å–å·¥ä½œæ—¥å¿—
    log_file = Path("/tmp/transcript.log")
    if not log_file.exists():
        raise FileNotFoundError("æ‰¾ä¸åˆ°å·¥ä½œæ—¥å¿—æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œtranscriptå‘½ä»¤")

    with open(log_file, "r", encoding="utf-8") as f:
        log = json.load(f)
        name = log["name"]
        video = Path(log["raw_video"])
        parent = Path(log["working_dir"])

    # æŸ¥æ‰¾å­—å¹•æ–‡ä»¶ - ä¼˜å…ˆæŸ¥æ‰¾é¡¹ç›®æ ¹ç›®å½•
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    srt_file = project_root / f"{name}.srt"

    if not srt_file.exists():
        # å°è¯•åœ¨å·¥ä½œç›®å½•æŸ¥æ‰¾
        srt_file = parent / f"{name}.srt"
        if not srt_file.exists():
            # å°è¯•åœ¨å½“å‰ç›®å½•æŸ¥æ‰¾
            srt_file = Path(f"./{name}.srt").resolve()
            if not srt_file.exists():
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°å­—å¹•æ–‡ä»¶: {srt_file}")

    if not video.exists():
        # å°è¯•åœ¨å·¥ä½œç›®å½•æŸ¥æ‰¾è§†é¢‘æ–‡ä»¶
        video_in_workspace = parent / Path(video).name
        if video_in_workspace.exists():
            video = video_in_workspace
        else:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è§†é¢‘æ–‡ä»¶: {video}")

    print(f"å¤„ç†å­—å¹•æ–‡ä»¶: {srt_file}")
    print(f"å¤„ç†è§†é¢‘æ–‡ä»¶: {video}")

    workspace = parent / "cut"
    workspace.mkdir(exist_ok=True)

    out_srt = parent / "cut.srt"

    custom_map = init_jieba()
    markers = "å¥½å‘ƒæ©å—¯"

    to_del = []

    subs = pysubs2.load(str(srt_file))
    keep_subs = pysubs2.SSAFile()
    cum_lag: int = 0

    def remove_event(to_del: list, i: int, event: Any) -> int:
        # é‡åˆ°è¿ç»­åˆ é™¤ï¼Œè¦è¿›è¡Œåˆå¹¶
        if len(to_del) > 0 and to_del[-1][0] == i - 1:
            item = to_del[-1]
            item[0] = i
            lag = event.end - item[2]
            item[2] = event.end
            return lag
        else:
            to_del.append([i, event.start, event.end])
            return event.duration

    deleted_count = 0
    for i, event in enumerate(subs.events):
        text = event.text
        if len(text) == 1 and text in markers:
            cum_lag += remove_event(to_del, i, event)
            deleted_count += 1
            print(f"åˆ é™¤è¯­åŠ©è¯: {event.text}")
            continue

        if text.startswith("[del]") or text.startswith("[DEL]"):
            cum_lag += remove_event(to_del, i, event)
            deleted_count += 1
            print(f"åˆ é™¤æ ‡è®°å­—å¹•: {event.text}")
            continue

        replaced = "".join([custom_map.get(x, x) for x in jieba.cut(text)])
        if event.text != replaced:
            print(f"è¯å…¸çº é”™: {event.text} -> {replaced}")

        event.text = replaced
        event.start -= cum_lag
        event.end -= cum_lag
        keep_subs.events.append(event)

    keep_subs.save(str(out_srt))
    print(f"åˆ é™¤äº† {deleted_count} ä¸ªå­—å¹•ç‰‡æ®µ")
    print(f"ä¿ç•™äº† {len(keep_subs.events)} ä¸ªå­—å¹•ç‰‡æ®µ")

    # åˆ‡åˆ†è§†é¢‘
    if to_del:
        print("å¼€å§‹åˆ‡åˆ†è§†é¢‘...")
        cut_video(video, to_del, workspace)
        print("å­—å¹•åˆ‡åˆ†å®Œæˆï¼Œå¼€å§‹åˆå¹¶å’Œå‹ç¼©")
    else:
        print("æ²¡æœ‰éœ€è¦åˆ é™¤çš„ç‰‡æ®µï¼Œç›´æ¥è¿›è¡Œåˆå¹¶")
        # åˆ›å»ºä¸€ä¸ªåŒ…å«å®Œæ•´è§†é¢‘çš„åˆ—è¡¨æ–‡ä»¶
        list_file = workspace / "list.text"
        with open(list_file, "w", encoding="utf-8") as f:
            f.write(f"file '{video}'\n")

    return parent


def adjust_subtitles_offset(srt: Path, opening_video: Path, full_srt: Path):
    # dur_ms = int(round(probe_duration(opening_video) * 1000))
    cmd = [
        "ffprobe",
        "-v", "error",
        "-show_entries", "format=duration",
        "-of", "default=noprint_wrappers=1:nokey=1",
        opening_video,
    ]
    dur_ms = int(round(float(subprocess.check_output(cmd).strip()) * 1000))

    subs = pysubs2.load(str(srt))
    for event in subs.events:
        event.start += dur_ms
        event.end += dur_ms

    subs.save(str(full_srt))


def optimize_subtitles_for_display(srt_file: Path, output_srt: Path = None):
    """
    ä¼˜åŒ–å­—å¹•æ˜¾ç¤ºï¼šè‡ªåŠ¨æ¢è¡Œå’Œè°ƒæ•´è¿‡é•¿çš„å­—å¹•

    Args:
        srt_file: è¾“å…¥å­—å¹•æ–‡ä»¶
        output_srt: è¾“å‡ºå­—å¹•æ–‡ä»¶ï¼Œå¦‚æœä¸ºNoneåˆ™è¦†ç›–åŸæ–‡ä»¶
    """
    if output_srt is None:
        output_srt = srt_file

    subs = pysubs2.load(str(srt_file))

    for event in subs.events:
        text = event.text.strip()
        if not text:
            continue

        # è®¡ç®—å­—ç¬¦é•¿åº¦ï¼ˆä¸­æ–‡å­—ç¬¦æŒ‰2ä¸ªå­—ç¬¦è®¡ç®—ï¼‰
        char_count = sum(2 if ord(c) > 127 else 1 for c in text)

        # æ ¹æ®å­—ç¬¦é•¿åº¦å†³å®šæ˜¯å¦éœ€è¦æ¢è¡Œ
        if char_count > 40:  # è¶…è¿‡40ä¸ªå­—ç¬¦å®½åº¦éœ€è¦æ¢è¡Œ
            # æ™ºèƒ½æ¢è¡Œï¼šä¼˜å…ˆåœ¨æ ‡ç‚¹ç¬¦å·å¤„æ¢è¡Œ
            punctuation = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€'
            words = []
            current_word = ""

            for char in text:
                current_word += char
                if char in punctuation:
                    words.append(current_word)
                    current_word = ""

            if current_word:
                words.append(current_word)

            # é‡æ–°ç»„ç»‡æˆä¸¤è¡Œ
            if len(words) > 1:
                mid_point = len(words) // 2
                line1 = ''.join(words[:mid_point]).strip()
                line2 = ''.join(words[mid_point:]).strip()

                # ç¡®ä¿ä¸¤è¡Œé•¿åº¦ç›¸å¯¹å‡è¡¡
                line1_len = sum(2 if ord(c) > 127 else 1 for c in line1)
                line2_len = sum(2 if ord(c) > 127 else 1 for c in line2)

                if abs(line1_len - line2_len) > 10 and len(words) > 2:
                    # é‡æ–°è°ƒæ•´åˆ†å‰²ç‚¹
                    best_split = mid_point
                    best_diff = abs(line1_len - line2_len)

                    for i in range(1, len(words)):
                        test_line1 = ''.join(words[:i]).strip()
                        test_line2 = ''.join(words[i:]).strip()
                        test_len1 = sum(2 if ord(c) > 127 else 1 for c in test_line1)
                        test_len2 = sum(2 if ord(c) > 127 else 1 for c in test_line2)
                        test_diff = abs(test_len1 - test_len2)

                        if test_diff < best_diff:
                            best_diff = test_diff
                            best_split = i

                    line1 = ''.join(words[:best_split]).strip()
                    line2 = ''.join(words[best_split:]).strip()

                if line1 and line2:
                    event.text = f"{line1}\\N{line2}"
            else:
                # å¦‚æœæ²¡æœ‰æ ‡ç‚¹ç¬¦å·ï¼ŒæŒ‰å­—ç¬¦æ•°å¼ºåˆ¶æ¢è¡Œ
                mid_char = len(text) // 2
                # å¯»æ‰¾æœ€è¿‘çš„ç©ºæ ¼æˆ–åˆé€‚çš„æ–­ç‚¹
                split_point = mid_char
                for i in range(max(0, mid_char - 5), min(len(text), mid_char + 5)):
                    if text[i] in ' \t':
                        split_point = i
                        break

                line1 = text[:split_point].strip()
                line2 = text[split_point:].strip()
                if line1 and line2:
                    event.text = f"{line1}\\N{line2}"

    subs.save(str(output_srt))
    print(f"å­—å¹•æ˜¾ç¤ºä¼˜åŒ–å®Œæˆ: {output_srt}")


def get_adaptive_subtitle_style(srt_file: Path):
    """
    æ ¹æ®å­—å¹•å†…å®¹ç”Ÿæˆè‡ªé€‚åº”çš„å­—å¹•æ ·å¼

    Args:
        srt_file: å­—å¹•æ–‡ä»¶è·¯å¾„

    Returns:
        str: FFmpegå­—å¹•æ ·å¼å­—ç¬¦ä¸²
    """
    subs = pysubs2.load(str(srt_file))

    # åˆ†æå­—å¹•ç‰¹å¾
    max_char_count = 0
    avg_char_count = 0
    total_events = 0

    for event in subs.events:
        if event.text.strip():
            # è®¡ç®—å­—ç¬¦é•¿åº¦ï¼ˆä¸­æ–‡å­—ç¬¦æŒ‰2ä¸ªå­—ç¬¦è®¡ç®—ï¼‰
            char_count = sum(2 if ord(c) > 127 else 1 for c in event.text)
            max_char_count = max(max_char_count, char_count)
            avg_char_count += char_count
            total_events += 1

    if total_events > 0:
        avg_char_count /= total_events

    # æ ¹æ®å­—å¹•é•¿åº¦åŠ¨æ€è°ƒæ•´å­—ä½“å¤§å°
    if max_char_count > 60:
        font_size = 18  # å¾ˆé•¿çš„å­—å¹•ç”¨å°å­—ä½“
    elif max_char_count > 40:
        font_size = 20  # è¾ƒé•¿çš„å­—å¹•ç”¨ä¸­ç­‰å­—ä½“
    elif avg_char_count > 30:
        font_size = 22  # å¹³å‡è¾ƒé•¿ç”¨ç¨å°å­—ä½“
    else:
        font_size = 24  # çŸ­å­—å¹•ç”¨æ­£å¸¸å­—ä½“

    # ç”Ÿæˆæ ·å¼å­—ç¬¦ä¸² - é»‘è‰²å­—ä½“ç™½è‰²è¾¹æ¡†æ–¹æ¡ˆ
    style = (
        f"FontName=WenQuanYi Micro Hei Light,"
        f"FontSize={font_size},"
        f"PrimaryColour=&H00000000,"  # é»‘è‰²å­—ä½“
        f"OutlineColour=&H00FFFFFF," # ç™½è‰²æè¾¹
        f"Outline=10,"                # 10pxç™½è‰²æè¾¹
        f"Shadow=0,"                 # ä¸ä½¿ç”¨é˜´å½±
        f"MarginV=40,"               # åº•éƒ¨è¾¹è·
        f"MarginL=60,"               # å·¦è¾¹è·
        f"MarginR=60,"               # å³è¾¹è·
        f"Alignment=2,"              # åº•éƒ¨å±…ä¸­å¯¹é½
        f"WrapStyle=2"               # æ™ºèƒ½æ¢è¡Œ
    )

    print(f"è‡ªé€‚åº”å­—å¹•æ ·å¼: å­—ä½“å¤§å°={font_size}, æœ€é•¿å­—å¹•={max_char_count}å­—ç¬¦, å¹³å‡é•¿åº¦={avg_char_count:.1f}å­—ç¬¦")
    return style

def merge(output_path: Path = None,
          opening_video_path: Path = None, ending_video_path: Path = None):
    """åˆå¹¶å‰ªè¾‘ç‰‡æ®µã€ç‰‡å¤´ã€ç‰‡å°¾ä»¥åŠå‹ç¼©

    Args:
        working_dir: å·¥ä½œç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™ä»/tmp/transcript.logæ–‡ä»¶è¯»å–
        output_path: è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨åŸè§†é¢‘ä½ç½®
        opening_video_path: ç‰‡å¤´è§†é¢‘è·¯å¾„
        ending_video_path: ç‰‡å°¾è§†é¢‘è·¯å¾„

    è¿™ä¸€æ­¥ä¹‹åï¼Œæ‰€æœ‰çš„å­—å¹•åŠå‰ªè¾‘éƒ½åº”è¯¥æ­£ç¡®å®Œæˆäº†
    """
    # è¯»å–å·¥ä½œæ—¥å¿—
    log_file = Path("/tmp/transcript.log")
    if not log_file.exists():
        raise FileNotFoundError("æ‰¾ä¸åˆ°å·¥ä½œæ—¥å¿—æ–‡ä»¶")


    log = json.load(open(log_file, "r", encoding="utf-8"))
    working_dir = Path(log["working_dir"])
    name = log["name"]
    original_video = Path(log["raw_video"])

    cut_srt = working_dir / "cut.srt"
    if not cut_srt.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°å‰ªè¾‘åçš„å­—å¹•æ–‡ä»¶: {cut_srt}")

    main_video = working_dir / "main.mp4"
    list_file = working_dir / "cut/list.text"

    if not list_file.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è§†é¢‘ç‰‡æ®µåˆ—è¡¨æ–‡ä»¶: {list_file}")

    print("åˆå¹¶è§†é¢‘ç‰‡æ®µ...")
    command = f"ffmpeg -hide_banner -f concat -safe 0 -i {list_file} -fflags +genpts -c copy -map 0:v -map 0:a -movflags +faststart -video_track_timescale 600 -f mp4 -avoid_negative_ts make_zero -v error -y '{main_video}'"

    # merge main video
    execute(command, msg="åˆå¹¶ä¸»è§†é¢‘")

    # å‡†å¤‡æœ€ç»ˆè§†é¢‘æ–‡ä»¶åˆ—è¡¨
    video_files = []

    # æ·»åŠ ç‰‡å¤´
    if opening_video_path and Path(opening_video_path).exists():
        video_files.append(Path(opening_video_path))
        print(f"æ·»åŠ ç‰‡å¤´è§†é¢‘: {opening_video_path}")

    # æ·»åŠ ä¸»è§†é¢‘
    video_files.append(main_video)

    # æ·»åŠ ç‰‡å°¾
    if ending_video_path and Path(ending_video_path).exists():
        video_files.append(Path(ending_video_path))
        print(f"æ·»åŠ ç‰‡å°¾è§†é¢‘: {ending_video_path}")

    # å¦‚æœæœ‰ç‰‡å¤´æˆ–ç‰‡å°¾ï¼Œéœ€è¦é‡æ–°åˆå¹¶
    if len(video_files) > 1:
        merge_list = working_dir / "full.text"
        merged_video = working_dir / f"full.mp4"

        with open(merge_list, "w", encoding="utf-8") as f:
            for video_file in video_files:
                f.write(f"file '{video_file}'\n")

        # é¦–å…ˆå°è¯•ä½¿ç”¨copyæ¨¡å¼åˆå¹¶ï¼Œé¿å…é‡æ–°ç¼–ç 
        cmd_copy = f"ffmpeg -hide_banner -f concat -safe 0 -i {merge_list} -fflags +genpts -c copy -map 0:v -map 0:a -movflags +faststart -y -f mp4 -video_track_timescale 600 -v error {merged_video}"

        try:
            execute(cmd_copy, msg="åˆå¹¶å®Œæ•´è§†é¢‘(copyæ¨¡å¼)")
        except RuntimeError:
            print("âš ï¸ copyæ¨¡å¼å¤±è´¥ï¼Œå°è¯•é‡æ–°ç¼–ç ...")
            # å¦‚æœcopyæ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨é‡æ–°ç¼–ç 
            cmd_encode = f"ffmpeg -hide_banner -f concat -safe 0 -i {merge_list} -fflags +genpts -c:v libx264 -preset medium -crf 23 -c:a aac -b:a 128k -map 0:v -map 0:a -movflags +faststart -y -f mp4 -video_track_timescale 600 -v error {merged_video}"
            execute(cmd_encode, msg="åˆå¹¶å®Œæ•´è§†é¢‘(é‡æ–°ç¼–ç )")

        # å¦‚æœæœ‰ç‰‡å¤´ï¼Œéœ€è¦è°ƒæ•´å­—å¹•æ—¶é—´åç§»
        if opening_video_path:
            adjusted_srt = working_dir / "adjusted.srt"
            adjust_subtitles_offset(cut_srt, opening_video_path, adjusted_srt)
            cut_srt = adjusted_srt
    else:
        merged_video = main_video

    # è‡ªåŠ¨å¯¹é½å­—å¹• - è¿™æ˜¯å…³é”®æ­¥éª¤
    print("å¼€å§‹å­—å¹•å¯¹é½...")
    aligned_srt = working_dir / "aligned.srt"
    align_subtitles_with_audio(merged_video, cut_srt, aligned_srt)

    # ç¡®å®šæœ€ç»ˆè¾“å‡ºè·¯å¾„
    if output_path is None:
        # ä½¿ç”¨åŸè§†é¢‘çš„ç›®å½•ï¼Œæ–‡ä»¶ååŠ åç¼€
        output_dir = original_video.parent
        base_name = original_video.stem
    else:
        output_path = Path(output_path)
        if output_path.is_dir():
            output_dir = output_path
            base_name = name
        else:
            output_dir = output_path.parent
            base_name = output_path.stem

    # ç”Ÿæˆæœ€ç»ˆæ–‡ä»¶è·¯å¾„
    final_with_sub = output_dir / f"{base_name}-final-sub.mp4"
    final_no_sub = output_dir / f"{base_name}-final.mp4"
    final_srt = output_dir / f"{base_name}-final.srt"

    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    print(f"å¸¦å­—å¹•ç‰ˆæœ¬: {final_with_sub}")
    print(f"æ— å­—å¹•ç‰ˆæœ¬: {final_no_sub}")

    # ä¼˜åŒ–å­—å¹•æ˜¾ç¤º
    print("ä¼˜åŒ–å­—å¹•æ˜¾ç¤º...")
    optimized_srt = working_dir / "optimized.srt"
    optimize_subtitles_for_display(aligned_srt, optimized_srt)

    # å‹ç¼©åŠçƒ§å­—å¹•
    print("ç”Ÿæˆå¸¦å­—å¹•ç‰ˆæœ¬...")

    # è·å–è‡ªé€‚åº”å­—å¹•æ ·å¼
    subtitle_style = get_adaptive_subtitle_style(optimized_srt)

    cmd = f"ffmpeg -hide_banner -i {merged_video} -vf \"subtitles={optimized_srt}:force_style='{subtitle_style}'\" -c:v libx264 -preset slow -crf 23 -c:a copy -v error -y '{final_with_sub}'"
    execute(cmd, msg="ç”Ÿæˆå¸¦å­—å¹•ç‰ˆæœ¬")

    # å‹ç¼©æœªåŠ å­—å¹•çš„è§†é¢‘
    print("ç”Ÿæˆæ— å­—å¹•ç‰ˆæœ¬...")
    cmd = f"ffmpeg -hide_banner -i {merged_video} -c:v libx264 -preset slow -crf 23 -c:a copy -v error -y '{final_no_sub}'"
    execute(cmd, msg="ç”Ÿæˆæ— å­—å¹•ç‰ˆæœ¬")

    # å¤åˆ¶å­—å¹•æ–‡ä»¶
    shutil.copy2(aligned_srt, final_srt)

    print("\n=== å¤„ç†å®Œæˆ ===")
    print(f"âœ… å¸¦å­—å¹•è§†é¢‘: {final_with_sub}")
    print(f"âœ… æ— å­—å¹•è§†é¢‘: {final_no_sub}")
    print(f"âœ… å­—å¹•æ–‡ä»¶: {final_srt}")

    return final_with_sub, final_no_sub, final_srt


def t2s(srt: str):
    """å°†ç¹ä¸­è½¬æ¢ä¸ºç®€ä¸­"""
    srt_file = Path(srt)
    if not srt_file.exists():
        raise FileNotFoundError(f"å­—å¹•æ–‡ä»¶ä¸å­˜åœ¨: {srt_file}")

    subs = pysubs2.load(str(srt_file))

    converter = opencc.OpenCC("t2s.json")
    for event in subs.events:
        text = event.text

        replaced = converter.convert(text)
        if event.text != replaced:
            print(f"{event.text} -> {replaced}")
        event.text = replaced

    subs.save(str(srt_file))

def process_video(input_video: str, output_path: str = None,
                 opening_video: str = None, ending_video: str = None,
                 auto_process: bool = False):
    """
    å®Œæ•´çš„è§†é¢‘å¤„ç†æµç¨‹

    Args:
        input_video: è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆç›®å½•æˆ–æ–‡ä»¶è·¯å¾„ï¼‰
        opening_video: ç‰‡å¤´è§†é¢‘è·¯å¾„
        ending_video: ç‰‡å°¾è§†é¢‘è·¯å¾„
        auto_process: æ˜¯å¦è‡ªåŠ¨å¤„ç†ï¼ˆè·³è¿‡æ‰‹åŠ¨ç¼–è¾‘æ­¥éª¤ï¼‰
    """
    try:
        print("=== å¼€å§‹è§†é¢‘å­—å¹•å¤„ç†æµç¨‹ ===")

        # æ­¥éª¤1: ç”Ÿæˆå­—å¹•
        print("\næ­¥éª¤1: ç”Ÿæˆå­—å¹•")
        srt_file = transcript(Path(input_video))

        if not auto_process:
            print(f"\nè¯·ç¼–è¾‘å­—å¹•æ–‡ä»¶: {srt_file}")
            print("- å¯ä»¥åˆ é™¤ä¸éœ€è¦çš„å­—å¹•è¡Œ")
            print("- åœ¨éœ€è¦åˆ é™¤çš„å­—å¹•å‰æ·»åŠ  [DEL] æ ‡è®°")
            print("- ç¼–è¾‘å®ŒæˆåæŒ‰å›è½¦ç»§ç»­...")
            input("æŒ‰å›è½¦é”®ç»§ç»­...")

        # æ­¥éª¤2: å‰ªè¾‘è§†é¢‘
        print("\næ­¥éª¤2: å‰ªè¾‘è§†é¢‘")
        cut()

        # æ­¥éª¤3: åˆå¹¶å’Œè¾“å‡º
        print("\næ­¥éª¤3: åˆå¹¶å’Œè¾“å‡º")
        final_with_sub, final_no_sub, final_srt = merge(
            output_path=Path(output_path) if output_path else None,
            opening_video_path=Path(opening_video) if opening_video else None,
            ending_video_path=Path(ending_video) if ending_video else None
        )

        print("\n=== å¤„ç†å®Œæˆ ===")
        return final_with_sub, final_no_sub, final_srt

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        raise


def resume(output_path: str = None, opening_video: str = None, ending_video: str = None):
    """
    ç¼–è¾‘å­—å¹•åç»§ç»­å¤„ç†ï¼ˆè‡ªåŠ¨è°ƒç”¨alignï¼‰

    Args:
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆç›®å½•æˆ–æ–‡ä»¶è·¯å¾„ï¼‰
        opening_video: ç‰‡å¤´è§†é¢‘è·¯å¾„
        ending_video: ç‰‡å°¾è§†é¢‘è·¯å¾„
    """
    try:
        print("=== ç»§ç»­å¤„ç†å·²ç¼–è¾‘çš„å­—å¹• ===")
        print("å°†è‡ªåŠ¨è°ƒç”¨å­—å¹•å¯¹é½åŠŸèƒ½")

        # æ­¥éª¤1: å‰ªè¾‘è§†é¢‘
        print("\næ­¥éª¤1: å‰ªè¾‘è§†é¢‘")
        cut()

        # æ­¥éª¤2: åˆå¹¶å’Œè¾“å‡ºï¼ˆmergeå‡½æ•°å†…éƒ¨ä¼šè‡ªåŠ¨è°ƒç”¨alignï¼‰
        print("\næ­¥éª¤2: åˆå¹¶å’Œè¾“å‡º")
        final_with_sub, final_no_sub, final_srt = merge(
            output_path=Path(output_path) if output_path else None,
            opening_video_path=Path(opening_video) if opening_video else None,
            ending_video_path=Path(ending_video) if ending_video else None
        )

        print("\n=== å¤„ç†å®Œæˆ ===")
        print(f"âœ… å¸¦å­—å¹•è§†é¢‘: {final_with_sub}")
        print(f"âœ… æ— å­—å¹•è§†é¢‘: {final_no_sub}")
        print(f"âœ… å­—å¹•æ–‡ä»¶: {final_srt}")

        return final_with_sub, final_no_sub, final_srt

    except Exception as e:
        print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        raise


def auto(input_video: str, output_path: str = None,
         opening_video: str = None, ending_video: str = None):
    """
    è‡ªåŠ¨å¤„ç†æµç¨‹ï¼ˆæ— éœ€æ‰‹åŠ¨ç¼–è¾‘å­—å¹•ï¼‰

    Args:
        input_video: è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„ï¼ˆç›®å½•æˆ–æ–‡ä»¶è·¯å¾„ï¼‰
        opening_video: ç‰‡å¤´è§†é¢‘è·¯å¾„
        ending_video: ç‰‡å°¾è§†é¢‘è·¯å¾„
    """
    return process_video(input_video, output_path, opening_video, ending_video, auto_process=True)


def test():
    """æµ‹è¯•æ¨¡å‹åŠ è½½åŠŸèƒ½"""
    import os
    print("HF_ENDPOINT:", os.environ.get("HF_ENDPOINT"))
    print("Model directory:", model_dir)

    # è®¾ç½®ç¦»çº¿æ¨¡å¼
    os.environ["TRANSFORMERS_OFFLINE"] = "1"
    os.environ["HF_HUB_OFFLINE"] = "1"

    device = "cpu"
    try:
        print("Loading align model...")

        # å°è¯•ä½¿ç”¨æœ¬åœ°è·¯å¾„
        local_model_path = Path(model_dir) / "models--jonatasgrosman--wav2vec2-large-xlsr-53-chinese-zh-cn"
        if local_model_path.exists():
            # æŸ¥æ‰¾æœ€æ–°çš„å¿«ç…§ç›®å½•
            snapshots_dir = local_model_path / "snapshots"
            if snapshots_dir.exists():
                snapshot_dirs = [d for d in snapshots_dir.iterdir() if d.is_dir()]
                if snapshot_dirs:
                    latest_snapshot = max(snapshot_dirs, key=lambda x: x.stat().st_mtime)
                    print(f"Using local model from: {latest_snapshot}")
                    model_name = str(latest_snapshot)
                else:
                    model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn"
            else:
                model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn"
        else:
            model_name = "jonatasgrosman/wav2vec2-large-xlsr-53-chinese-zh-cn"

        model_a, metadata = whisperx.load_align_model(
            language_code="zh",
            device=device,
            model_name=model_name,
            model_dir=model_dir
        )
        print("âœ… Model loaded successfully!")
        print(f"Model type: {type(model_a)}")
        print(f"Metadata: {metadata}")
    except Exception as e:
        print(f"âŒ Model loading failed: {e}")
        print("Please check if the model files are properly downloaded.")

# CLIå…¥å£ç‚¹å·²ç§»è‡³ transcript/cli.py
# å¦‚éœ€ä½¿ç”¨åŸç‰ˆå‘½ä»¤ï¼Œè¯·ç›´æ¥å¯¼å…¥ç›¸åº”å‡½æ•°
